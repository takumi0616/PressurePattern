```

この混合確率的主成分分析とはどんな手法ですか？

混合確率的主成分分析（Mixtures of Probabilistic Principal Component Analyzers, MPPCA）は、クラスタリングと次元削減を同時に行うための機械学習の手法です。

この手法を理解するために、まず「主成分分析（PCA）」と「混合ガウスモデル（GMM）」の2つの要素に分解して考えてみましょう。

1. 確率的主成分分析 (Probabilistic PCA, PPCA) probabilstic

通常の主成分分析（PCA）は、データを最もよく表現するような低次元の軸（主成分）を見つけ出す手法ですが、確率的なモデルではありません。

それに対して、**確率的主成分分析（PPCA）**は、PCAを確率的なモデルとして再定式化したものです。具体的には、データが低次元の「潜在変数」と「ノイズ」から生成されるという生成モデルを仮定します。これにより、単にデータを変換するだけでなく、そのデータが生成される確率（尤度）を計算できるようになります。

2. 混合ガウスモデル (Gaussian Mixture Model, GMM)

GMMは、データ全体が複数の異なるガウス分布（釣鐘型の分布）の重ね合わせで表現できると考えるクラスタリング手法です。各ガウス分布がそれぞれ一つのクラスターに対応し、データ点がどのクラスターに属するかを確率的に推定します。

混合確率的主成分分析 (MPPCA) の仕組み

MPPCAは、これら2つのアイデアを組み合わせたものです。つまり、**「データは複数のクラスターから成り立っており、それぞれのクラスターは、独自の低次元の部分空間（主成分）を持っている」**と仮定します。

具体的には、以下の図のようにイメージできます。



データ全体が、複数のグループ（クラスター）に分かれているとします。（GMMの考え方）

それぞれのクラスター内部では、データはそのクラスターに固有の主成分に沿って分布しています。（PPCAの考え方）



このモデルを使うことで、以下のようなことが可能になります。



クラスタリング: 各データ点が、どのクラスターに属する確率が最も高いかを推定します。

局所的な次元削減: 各クラスターごとに、その特徴を最もよく表す主成分（低次元の軸）を抽出します。

MPPCAの利点

柔軟なモデリング: データ全体で単一の主成分を仮定するPCAとは異なり、クラスターごとに異なる主成分を持つことができるため、より複雑なデータ構造を捉えることができます。例えば、あるクラスターは細長く、別のクラスターは円形といったような、多様な形状のクラスターを同時に扱うことが可能です。

同時最適化: クラスタリングと次元削減を別々に行うのではなく、EMアルゴリズム（期待値最大化法）を用いて、クラスターの割り当てと各クラスターの主成分を同時に最適化します。

確率的なアプローチ: 各データがどのクラスターに属するかを確率で表現するため、結果の不確実性を評価することができます。

応用例

MPPCAは、その特性から以下のような分野で利用されています。



画像認識: 手書き文字認識などで、同じ数字でも書き手によって微妙に形が違う（＝異なる部分空間に属する）場合の特徴抽出と分類。

異常検知: 正常なデータを複数のクラスターとしてモデル化し、どのクラスターにも属さないデータを異常として検知する。

時系列データ解析: 異なる状態（レジーム）を持つ時系列データにおいて、各状態における主要な変動パターンを抽出する。

このように、MPPCAは単にデータを分類したり、次元を削減するだけでなく、データに内在する「局所的な構造」を捉えるための強力なツールと言えます。

この手法はクラスタ数を動的に変化できますか？















いいえ、標準的な混合確率的主成分分析（MPPCA）では、クラスター数を動的に（アルゴリズムの実行中に自動で）変化させることはできません。

MPPCAやその基礎となる混合ガウスモデル（GMM）では、モデルを学習させる前に、いくつのクラスターに分けるか（クラスター数 K）を人間が事前に指定する必要があります。

## 最適なクラスター数の決定方法

では、どうやって最適なクラスター数を決めるのでしょうか。一般的には、以下の方法が用いられます。



複数のクラスター数でモデルを試す: まず、クラスター数を2, 3, 4, ...といくつか候補を決めて、それぞれの数でMPPCAモデルを学習させます。

情報量基準で評価する: 次に、学習させた各モデルを「情報量基準」と呼ばれる指標で評価し、最も良いスコアを持つモデル（クラスター数）を選択します。代表的な指標には以下のものがあります。

BIC (ベイズ情報量基準): モデルのデータへの当てはまりの良さと、モデルの複雑さ（パラメーターの多さ）のバランスを評価します。モデルが複雑になるほどペナルティが大きくなるため、不必要に多いクラスター数を選ぶのを防いでくれます。一般的に、GMMやMPPCAのクラスター数決定でよく用いられます。

AIC (赤池情報量基準): BICと似ていますが、モデルの複雑さに対するペナルティがBICより小さいです。

このアプローチにより、データに対して「最も妥当な」クラスター数を客観的に選ぶことができます。

## 【発展】クラスター数を自動決定する手法

標準的な手法ではありませんが、クラスター数を自動で推定するための発展的なアプローチも存在します。

これはベイズノンパラメトリック法と呼ばれ、その代表的なモデルにディリクレ過程混合モデル (Dirichlet Process Mixture Model) があります。

この手法は、最初からクラスター数を固定せず、データに応じて必要な数のクラスターを自動的に生成していくような柔軟なモデリングが可能です。これは、MPPCAにも応用できますが、数学的に複雑で計算コストも高くなるため、より高度な分析で用いられます。

まとめ

標準的なMPPCA: クラスター数は事前に指定する必要がある。

最適なクラスター数の選択: BICなどの情報量基準を使って、複数の候補から最も良いものを後から選ぶのが一般的。

自動決定したい場合: ベイズノンパラメトリック法という発展的な手法がある。

このMPPCAはクラスタごとに異なる空間に配置されますか？

その場合、クラスタを1つの2次元空間にマッピングはできませんか？



はい、その2つのご質問にお答えします。非常に良い着眼点です。

1. MPPCAはクラスタごとに異なる空間に配置されますか？

はい、そのように考えることができます。より正確に言うと、MPPCAでは**「各クラスターが、それぞれ独自の低次元の部分空間（principal subspace）を持つ」**とモデル化します。



これは、データ全体が1つの大きな高次元空間に存在している中で、クラスターごとに異なる「主成分の軸」を持つことを意味します。

例えば、3次元空間に分布するデータがあったとします。あるクラスターは「細長い線」のような形状（1次元の部分空間）でモデル化され、別のクラスターは「平べったい円盤」のような形状（2次元の部分空間）でモデル化される、といったイメージです。

この「独自の軸を持つ」という点を、「クラスタごとに異なる空間に配置される」と捉えるのは、この手法の本質を理解する上で非常に分かりやすい表現です。

2. クラスタを1つの2次元空間にマッピングはできませんか？

いいえ、マッピングは可能です。 そして、それこそがMPPCAの結果を可視化する際の一般的なアプローチです。

少しややこしく聞こえるかもしれませんが、**「モデリング」の段階と「可視化」**の段階を分けて考えると分かりやすいです。



モデリング（MPPCAの計算）: 高次元の元データに対してMPPCAを適用し、「このデータ点はクラスターAに属する」「このデータ点はクラスターBに属する」といったように、各データ点の所属クラスターを決定します。このとき、内部的には各クラスターが持つ固有の部分空間が考慮されています。

可視化（2次元へのマッピング）: 次に、すべての元データ（高次元）に対して、PCAやt-SNEといった単一の次元削減手法を適用し、共通の2次元空間にプロットします。 そして、そのプロット上で、ステップ1で決定されたクラスターのラベル情報（クラスターA, B, ...）を使って、各データ点を色分けします。

まとめると…

MPPCAは、高次元空間における各クラスターの局所的な構造（異なる部分空間）を捉えて分類を行いますが、その結果を人間が理解できるように可視化する際には、すべてのデータを1つの共通の2次元マップに落とし込んで表示するのが一般的です。

これにより、「クラスターAとクラスターBは、高次元空間ではこういう関係性で分離されているのか」といったことを、2次元の散布図で直感的に把握することができます。

このMPPCAの出力を自己組織化マップに紐づけて可視化は可能ですか？















はい、可能です。MPPCAの出力と自己組織化マップ（SOM）を連携させるのは、データをより深く理解するための非常に優れたアプローチです。

両者の強みを組み合わせることで、単体で使うよりもリッチな可視化が実現できます。



MPPCA: 各クラスターが持つ固有の形状や方向性（部分空間）を捉えて、データを精密に分類します。

SOM: データ全体の類似性や繋がり（トポロジー）を保ったまま、2次元のマップ上に配置します。

## 可視化の具体的なステップ

最も一般的で効果的な方法は、以下の3ステップです。

ステップ1: MPPCAでクラスタリング

まず、高次元の元データに対してMPPCAを適用し、各データポイントがどのクラスターに属するかを決定します。これにより、すべてのデータに「クラスターA」「クラスターB」といったラベルが付きます。

ステップ2: SOMでマッピング

次に、同じ高次元の元データを使ってSOMを学習させます。SOMは、似たデータがマップ上の近い位置に来るように、すべてのデータを2次元のグリッド上に配置します。

ステップ3: 結果を統合して可視化

最後に、ステップ2で作成したSOMのマップを、ステップ1で得られたMPPCAのクラスターラベルを使って色分けします。

具体的には、SOMの各ノード（マス目）にマッピングされたデータポイントたちが、どのMPPCAクラスターに最も多く属しているかに基づいて、そのノードを彩色します。

## この手法で何がわかるか？

この連携により、単なるクラスターの分布図以上のことがわかります。



クラスター間の関係性: SOM上でMPPCAのクラスターが隣接していれば、それらのクラスターはデータ空間上でも近い性質を持つことがわかります。

クラスターの境界: あるクラスターから別のクラスターへの移り変わりが、マップ上でどのように起きているか（急に変わるのか、徐々に変わるのか）を視覚的に捉えられます。

データのトポロジー理解: MPPCAが見つけた「統計的に異なるグループ」が、データ全体の連続的な構造の中でどのような位置を占めているかを直感的に理解できます。

このように、MPPCAとSOMを組み合わせることで、データの局所的な構造（クラスター）と大域的な構造（トポロジー）を同時に可視化でき、より深い洞察を得ることが可能になります。

MPPCAから得られる各クラスタへの所属確率など何かしらのベクトルをSOMに入力することはできますか？















はい、それは素晴らしいアイデアです。MPPCAから得られる各クラスターへの所属確率ベクトルを、そのまま自己組織化マップ（SOM）に入力して可視化することは可能です。

これは、より一歩進んだ高度な分析手法と言えます。

## その手法の仕組みと得られるもの

このアプローチでは、データが2段階で変換・要約されることになります。

ステップ1: MPPCAで所属確率ベクトルを取得

まず、高次元の元データに対してMPPCAを適用します。そして、各データポイントに対して「最も可能性の高い単一のクラスター」を選ぶのではなく、「各クラスターに所属する確率」を並べたベクトルを抽出します。

例えば、クラスター数が3つの場合、あるデータ点Aは以下のようなベクトルで表現されます。



データ点A → [0.95, 0.04, 0.01] (クラスター1に強く所属)

データ点B → [0.51, 0.48, 0.01] (クラスター1と2の境界にいる)

この確率ベクトル自体が、元のデータを**「クラスターとの関係性」という観点から要約した新しい特徴量**と見なせます。

ステップ2: 所属確率ベクトルをSOMに入力

次に、この確率ベクトルの集まりをSOMに入力して学習させます。

## この可視化で何がわかるか？ 🗺️

この方法で作成したSOMは、単にデータを配置するだけでなく、**「MPPCAによるクラスタリングの確信度」**を可視化します。



クラスターの"核"の可視化: マップ上のある領域には、特定のクラスターへの所属確率が極めて高い（例: [0.99, 0.01, 0.00]）データポイントが集まります。これらは、そのクラスターの典型的なメンバー（核）と言えます。

クラスター間の"境界"の可視化: マップ上の別の領域、特に"核"となる領域の間には、複数のクラスターへの所属確率が混ざり合っている（例: [0.5, 0.5, 0.0]）データポイントが集まります。これらは、MPPCAが「どちらのクラスターに分類すべきか迷った」データであり、クラスター間の境界や重なりを明確に示しています。

この手法は、単に「どのクラスターに属するか」だけでなく、「どのくらい 확실하게属するのか」、そして**「どのクラスターとどのクラスターが似ていて、境界が曖昧なのか」**といった、より深い情報を引き出すための非常に強力な分析ツールとなります。

このMPPCAと自己組織化マップによる可視化について深く検討し考えてまとめてください。



高次元データ構造と不確実性の可視化：ハイブリッドMPPCA-SOMフレームワーク

はじめに

高次元性の課題

現代の科学技術、ビジネス、社会科学の各分野において、高次元データは遍在しています。しかし、人間の直感は本質的に2次元または3次元に限定されており、この「次元の呪い」はデータに内在する本質的な構造、クラスター、パターンを理解する上で深刻な障壁となります。効果的な可視化とは、単にデータをプロットすることではなく、その高次元空間における内在的構造を、解釈可能な低次元空間へと忠実に写像し、洞察を導き出すプロセスです [1, 2]。この課題に応えるため、数多くの次元削減および可視化手法が開発されてきましたが、それぞれに長所と短所が存在します。

主役の紹介

本レポートでは、この高次元データの可視化という難題に取り組むための、二つの強力な機械学習技術の相乗効果に焦点を当てます。



混合確率的主成分分析（MPPCA）: この手法は、古典的な主成分分析（PCA）を確率的フレームワークへと拡張し、複雑なデータ分布を局所的な線形部分空間の混合としてモデル化する、洗練されたモデルベースのクラスタリング技術です [3, 4]。データが単一のグローバルな構造に従うのではなく、複数の異なる「モード」や「状態」から生成されるという、より現実的な仮定に基づいています。

自己組織化マップ（SOM）: 神経科学に着想を得たこのアルゴリズムは、高次元データのトポロジー（近傍関係）を保持したまま、低次元（通常は2次元）のグリッド上に写像する能力で広く知られています [5, 6, 7]。その結果得られるマップは、データのクラスタリング構造を直感的に把握するための強力な可視化ツールとなります。

相乗効果という主題

本レポートの中心的な論点は、MPPCAの確率的厳密性とSOMの直感的なトポロジーマッピング能力を組み合わせることで、どちらか一方の手法単独では到達不可能なレベルの洞察を引き出すハイブリッドフレームワークが構築できる、という点にあります。このフレームワークは、単にクラスター構造を可視化するだけでなく、より重要なことに、モデル自身の「不確実性」—すなわち、各データ点がどのクラスターに属するかについてのモデルの「自信」—を可視化することを可能にします。本稿では、このハイブリッドアプローチの理論的基盤、詳細な方法論、そして得られる可視化結果の解釈方法を深く掘り下げ、現代のデータ可視化技術の展望の中に位置づけます。

第1章 確率的主成分分析とその混合モデル：理論的深掘り

1.1 古典的PCAから確率的PCAへ：パラダイムシフト

古典的PCAの限界

主成分分析（PCA）は、次元削減の礎となる技術であり、射影されたデータの分散を最大化する線形射影として定義されます [8, 9]。その有効性と簡便さから広く利用されていますが、その本質は幾何学的な操作であり、確率モデルに基づいているわけではありません。この非確率的な性質が、いくつかの重要な限界を生み出します。特に、データが複数のクラスターから構成される場合の混合モデルへの拡張や、欠損データの principled な取り扱いが困難であるという点が挙げられます [10, 11, 12]。

PPCAの潜在変数モデル

TippingとBishopによる独創的な研究は、PCAを確率的な潜在変数モデルとして再定式化することで、この限界を打破しました [10, 11]。このモデル（Probabilistic PCA, PPCA）では、観測データは、より低次元の潜在空間に存在する潜在変数から生成されると仮定されます。具体的には、ガウス事前分布に従う潜在変数が、まず高次元のデータ空間へ線形写像され、その後、等方的なガウスノイズが付加されるという生成プロセスを想定します [12, 13, 14]。この定式化は、因子分析と密接に関連していますが、ノイズモデルに制約がある点で異なります [4, 15]。

尤度の力

この確率的定式化の最も重要な帰結は、データに対する尤度関数 p(data∣model) を定義できる点にあります。尤度の導入は、 principled な統計的推論、ベイズ的アプローチ、そして本稿の主題である混合モデルの構成要素としての利用への扉を開く鍵となります [4, 12]。古典的PCAがデータの「記述」に留まるのに対し、PPCAはデータの「生成モデル」を提供するのです。

1.2 混合確率的主成分分析（MPPCA）

複雑なデータのモデル化

現実世界のデータは、単一のグローバルな線形モデルに従うことは稀です。多くの場合、データは複数の異なる構造を持つ部分集団から構成されています。混合確率的主成分分析（Mixture of Probabilistic Principal Component Analyzers, MPPCA）は、データを複数のPPCAモデルの混合としてモデル化することで、この問題に対処します [3, 16]。混合モデルの各成分（コンポーネント）が局所的な線形部分空間を表現するため、全体として複雑で非線形なデータの変動を、より単純な局所モデルの集合体として捉えることが可能になります [17]。

ソフトクラスタリングとしてのMPPCA

MPPCAは、「ソフト」クラスタリングアルゴリズムとして機能します。これは、各データ点に対して、いずれか一つのクラスターへの「ハード」な割り当てを行うのではなく、各混合成分に属する事後確率（「負担率」や「responsibility」とも呼ばれる）を計算するためです [3, 13]。この事後確率ベクトルは、そのデータ点がどのクラスターにどの程度の確信度で属するかという、モデルの「信念」を表現しています。

1.3 MPPCAのための期待値最大化（EM）フレームワーク

パラメータ推定

MPPCAモデルのパラメータ（各成分の混合重み、平均、射影行列、ノイズ分散）は、通常、期待値最大化（Expectation-Maximization, EM）アルゴリズムを用いて推定されます [4, 14]。EMアルゴリズムは、観測データからは直接解くことが難しい尤度最大化問題を、潜在変数を導入することで反復的に解くための強力な手法です。



E-ステップ（期待値計算）: このステップでは、現在のパラメータ推定値に基づき、各データ点が各混合成分に属する事後確率（負担率）を計算します [14, 18, 19]。これは、モデルが観測データを見て、そのデータがどの成分から生成された可能性が高いかを「推測」する段階に相当します。

M-ステップ（最大化）: このステップでは、E-ステップで計算された負担率を用いて、モデルのパラメータを更新します。この更新は、期待対数尤度を最大化するように行われます [14, 18, 19]。これは、E-ステップでの推測を元に、モデル自身をデータにより適合するように「修正」する段階です。

このE-ステップとM-ステップの反復は、尤度関数の局所的最大値に収束することが保証されています [14]。

1.4 モデル選択と複雑性

重要な問い

MPPCAを実用する上での重要な課題は、混合成分の数（K）と、各成分における潜在部分空間の次元（q）を最適に決定することです [16, 20]。これらのハイパーパラメータは、モデルの表現力と汎化性能に直接影響します。

情報量規準

このモデル選択タスクには、ベイズ情報量規準（BIC）が一般的に用いられます。しかし、標準的なBICは、サンプルサイズ全体を用いて各成分を罰するため、成分数が多いモデルに対して過度にペナルティを課す傾向があります。この問題に対処するため、各成分（アナライザ）をその有効サンプルサイズのみを用いて罰する階層的BIC（hBIC）が提案されており、より適切なモデル選択が可能になるとされています.[3]

自動化・階層的アプローチ

研究の中には、階層的なMPPCAモデルを自動的に構築する手法も存在します。これらのアプローチでは、統合分類尤度（integrated classification likelihood）のような規準を用いて、クラスターを分割すべきか、また階層を追加すべきかを判断し、その過程で保持すべき主成分の次元数も自動的に決定します [3, 16]。

PCAからPPCA、そしてMPPCAへの進化は、単なるアルゴリズムの改良ではなく、記述的な幾何学的手法から生成的な統計モデルへの根本的なパラダイムシフトを意味します。このシフトの論理的連鎖は以下の通りです。



古典的PCAは、データを見るための一つの「射影」、つまり視点を提供しますが、データがどのようにして生成されたかという「生成物語」は提供しません [8, 11]。

PPCAは、この生成物語を導入します。すなわち、潜在変数がサンプリングされ、線形変換され、ノイズが加わるというプロセスです [12]。この物語があるからこそ、モデルを与えられたときのデータの確率、すなわち尤度 

p(data∣model) を記述できます。

ガウス混合モデル（GMM）[21] やMPPCAのような混合モデルは、根本的にEMアルゴリズムに依存しています。

EMアルゴリズムのE-ステップでは、事後確率 p(cluster∣data) の計算が必要です。ベイズの定理によれば、これは尤度 p(data∣cluster) と事前確率 p(cluster) の積に比例します。

PPCAのような確率モデルによって提供される尤度 p(data∣cluster) がなければ、E-ステップを principled に実行することはできません。

したがって、MPPCAは単なる「クラスタリング付きPCA」ではなく、局所的に線形なデータ構造に対する混合モデルの統計的に首尾一貫した実現形態であり、この偉業は古典的PCAでは原理的に達成不可能なのです。

表1：PCA、PPCA、MPPCAの比較分析

特徴主成分分析（PCA）確率的主成分分析（PPCA）混合確率的主成分分析（MPPCA）中心原理射影データの分散最大化潜在変数モデルの最尤推定潜在変数混合モデルの最尤推定モデルタイプ幾何学的射影生成的確率モデル生成的確率モデルの混合想定データ構造単一のグローバルな線形多様体ガウスノイズを伴う単一のグローバルな線形多様体複数の局所的な線形多様体クラスタリング能力なし（前処理として利用）なし（単一成分）事後確率による内在的なソフトクラスタリング確率的基盤なしあり（ガウス尤度）あり（ガウス分布の混合）欠損データ処理直接的には不可可（EMアルゴリズム経由）可（EMアルゴリズム経由）主な限界グローバルな線形性、スケーリングへの感度単一のデータモードを想定モデル選択の複雑さ（K, q）

Google スプレッドシートにエクスポート

第2章 自己組織化マップ：トポロジー保持可視化の原理

2.1 アーキテクチャとアルゴリズムの基礎

自己組織化マップ（SOM、またはコホネンマップ）は、教師なし学習を用いて訓練される人工ニューラルネットワークの一種です [5, 6, 22]。その構造は通常、入力層と、2次元グリッド状に配置された出力ニューロン層から構成されます [1, 23]。SOMの学習プロセスは、競争と協調という二つの特徴的な段階で進行します。



競争学習: 各入力データベクトルが与えられると、出力層のニューロン群は、その入力ベクトルを最もよく表現する権利を「競争」します。最も入力ベクトルに近い重みベクトルを持つニューロンが「勝者」（Best Matching Unit, BMU）として選出されます。この近さは、通常ユークリッド距離によって測定されます [1, 24, 25]。

協調学習: BMUが決定されると、そのBMU自身の重みベクトルと、グリッド上でその近傍に位置するニューロンの重みベクトルが、現在の入力ベクトルに近づくように更新されます。更新の度合いは、BMUからの距離が遠くなるにつれて、また学習が進むにつれて減少します [6, 7, 25]。この協調的な更新こそが、後述するトポロジー保持の鍵となります。

2.2 トポロジー保持の本質

近傍関係の写像

SOMの究極的な目標は、近傍関係を保持する写像を行うことです [1, 7]。これは、高次元の入力空間で互いに近接しているデータ点が、2次元の出力グリッド上でも互いに近接するニューロンに写像されることを意味します [6, 26]。この特性により、高次元空間におけるデータの複雑な構造が、人間が直感的に理解できる2次元マップ上に展開されます。

近傍関数

このトポロジー保持は、近傍関数 h(t) によって実現されます。これは多くの場合、BMUを中心とするガウス関数として定義されます。この関数の影響範囲（半径）は、学習の進行に伴って徐々に縮小していきます。学習の初期段階では、広い範囲のニューロンが更新されることで大域的な自己組織化が促され、後期段階では、狭い範囲の更新によって局所的な推定の微調整が行われます [6, 7, 25]。

2.3 SOMの可視化と解釈

訓練済みのSOMに内在するクラスター構造を可視化するために、いくつかの手法が用いられます。



U-Matrix（統合距離行列）: これは最も一般的に用いられる可視化手法です。各ニューロンとその近傍ニューロンとの間の重みベクトルの平均距離を計算し、色や高さで表現します。U-Matrix上で値が高い領域（明るい色や「山」）は、近傍ニューロン間のギャップが大きいことを示し、クラスターの境界を示唆します。一方、値が低い領域（暗い色や「谷」）は、ニューロンが密集していることを示し、クラスターの内部を形成します [23, 24, 27]。

成分平面とヒットマップ: 他の可視化手法として、個々の入力変数がマップ上でどのように分布しているかを示す成分平面や、各ニューロンにいくつのデータ点が写像されたか（つまり、そのニューロンがBMUとなった回数）を示すヒットマップなどがあります [22, 28]。

2.4 標準SOMの批判的評価

その強力な可視化能力にもかかわらず、標準的なSOMにはいくつかの理論的限界が存在します。



発見的（ヒューリスティック）な性質: SOMアルゴリズムは、明確に定義されたコスト関数の最適化から導出されたものではなく、生物学的な着想に基づく発見的なアイデアから生まれています [29, 30, 31]。

確率的基盤の欠如: SOMはデータに対する確率密度を定義しません。これにより、客観的なモデル比較や、統計的検定が困難になります [30, 32]。

パラメータへの感度: その性能は、グリッドサイズ、トポロジー（矩形か六角形か）、学習率のスケジュール、近傍関数のスケジュールといったハイパーパラメータの選択に大きく依存します [24, 33]。

収束性: 標準的なアルゴリズムに対する一般的な収束証明は存在しません [31, 34]。

SOMの「トポロジー保持」は強力な可視化コンセプトですが、それは定量的な特性ではなく、定性的な特性です。この区別は、その限界と、なぜ確率モデルとのハイブリッド化が求められるのかを理解する上で根本的に重要です。



SOMの目標は、高次元データのトポロジーを反映した低次元マップを作成することです [7]。これは人間の解釈にとって非常に有用です。

しかし、そのプロセスはアルゴリズム的かつ発見的です [29]。データ尤度に関連するグローバルなコスト関数を最適化しているわけではありません [30]。

その結果、「これら二つのニューロン群は分離している」とは言えても、マップ上の距離を用いて、統計的に厳密な意味で「どれだけ」分離しているかを言うことはできません。マップはトポロジー的順序を達成するために、空間を非線形に伸縮させることができます。

これは、SOMがデータの形式的な「確率モデル」ではなく、「表現」を提供することを意味します。これは決定的な限界です。SOMはクラスターが「存在すること」を示すことはできますが、新しい点が特定のクラスターに属する確率を教えたり、モデル自身のマップに対する信頼度を定量化したりすることはできません。

この限界こそが、MPPCAのような、欠けている確率的基盤を提供できるモデルとの統合を推進する主要な動機となります。SOMは視覚的なキャンバスを提供し、MPPCAは統計的に意味のある「絵の具」を提供するのです。

第3章 相乗効果フレームワーク：MPPCAとSOMの統合による高度な可視化

3.1 統合の論理的根拠

強みの組み合わせ

このハイブリッドフレームワークの核となるアイデアは、両手法の相補的な強みを活用することにあります。MPPCAは、局所的な次元性や不確実性を扱うための principled な方法と、頑健なモデルベースのクラスタリングを提供します [3, 17]。一方、SOMは、結果として得られるクラスターのトポロジー構造を、低次元で解釈しやすいグリッド上に可視化するための比類なき手法を提供します [35, 36]。

単純なパイプラインを超えて

この組み合わせは、単に「次元削減してから可視化する」という逐次的な処理以上の意味を持ちます。それは、MPPCAが生成するリッチな確率的出力を、SOMが組織化するための、より意味深く洞察に富んだ入力空間として利用することにあります [32, 35, 37]。

3.2 詳細な方法論的ワークフロー

MPPCAとSOMを統合するための具体的なワークフローは、以下のステップで構成されます。



データの前処理: 距離や分散に基づく他の多くの手法と同様に、入力データの標準化（例えば、平均0、標準偏差1への変換）は、すべての変数が等しく寄与することを保証するための重要な最初のステップです [38]。

MPPCAモデルの適合: 前処理されたデータに対してMPPCAモデルを訓練します。成分数（K）や潜在次元（q）などのハイパーパラメータは、BICや交差検証といった手法を用いて選択します [3, 39]。

キー情報の抽出: 訓練済みのMPPCAモデルから、各データ点 xn

​ に対して、事後確率のベクトル Pn

​=[p(c=1∣xn

​),p(c=2∣xn

​),…,p(c=K∣xn

​)] を抽出します。この K 次元のベクトルは、もはや元の特徴空間ではなく、モデルによって定義された「信念空間（belief space）」においてデータ点を表現します [14, 40]。

事後確率に対するSOMの訓練: この事後確率ベクトルの行列 P を、SOMを訓練するための入力データセットとして使用します。これにより、SOMはこれらの「信念プロファイル」を写像し、組織化することを学習します。類似した確率的割り当てを持つデータ点（例えば、クラスター3に属する確信度が高い点や、クラスター1と2の間で曖昧性が高い点など）は、マップ上の近接したニューロンに写像されることになります [32]。

射影と可視化: 最後に、元のデータ点を訓練済みSOM上に射影します。これは、各データ点の事後確率ベクトルに対するBMUを見つけることで行われます。結果として得られるマップは、後述する様々な手法を用いて可視化されます。

3.3 不確実性の可視化：事後確率マップ

コンセプト

これこそが、このハイブリッドフレームワークが可能にする斬新な可視化です。単に距離を示すU-Matrixの代わりに、マップ全体にわたるモデルの「自信」を直接可視化することができます。

実装

訓練済みSOM上の各ニューロン j に対して、そのニューロン j がBMUとなるすべてのデータ点 xn

​ の平均事後確率ベクトルを計算します。これにより、各ニューロンは自身の特徴的な「信念プロファイル」を持つことになります。

可視化技術

クラスターごとの成分平面: K 個のMPPCAクラスターそれぞれに対して、個別のヒートマップを作成します。k 番目のマップは、各ニューロンがクラスター k に属する平均事後確率を示します。これにより、特定のクラスターが支配的なマップ上の領域が明確にハイライトされます。

勝者総取り（Winner-Takes-All）マップ: 各ニューロンを、その平均事後確率が最も高いクラスターの色で塗り分ける単純な可視化です。これにより、主要なクラスターの「領土」が一目でわかります。

エントロピー／信頼度マップ: 各ニューロンの平均事後確率ベクトルのエントロピーを計算します。エントロピーが高い場合は不確実性が高いこと（確率が均等に分布している）を示し、エントロピーが低い場合は信頼度が高いこと（一つの確率が1に近い）を示します。これは、モデルの曖昧な領域を直接的に可視化するものです [40, 41]。

MPPCAとSOMの最も深遠な相乗効果は、MPPCAを特徴抽出に用いることによってではなく、データを事後確率の「信念空間」に変換するために用いることによって達成されます。これにより、SOMのタスクは、幾何学的な組織化からモデルの不確実性の組織化へと再定義されます。



標準的なアプローチは、PCA/MPPCAを用いて潜在変数を取得し、それをSOMに入力することです [35, 36]。これは有効な特徴量エンジニアリングのパイプラインです。

しかし、このアプローチはMPPCAモデルの最もリッチな出力、すなわち事後確率 p(c∣x) を捨ててしまいます。これらの確率は、個々のデータ点に対するモデルの信頼度と曖昧性を内包しています。

事後確率ベクトルをSOMの入力として用いることで、我々はSOMに、データそのものではなく、データに関する「モデルの信念」の中にトポロジー構造を見出すよう依頼していることになります [32]。

データ点はもはや (特徴1, 特徴2,...) ではなく、(クラスタ1の確率, クラスタ2の確率,...) となります。

したがって、SOMは「確信度の高いクラスタ1の点」、「確信度の高いクラスタ2の点」、そして決定的に重要なことに、「クラスタ1と2の間で曖昧な点」を、それぞれ別個の領域に自然にグループ化します。

これにより、SOMは単なるデータ可視化ツールから、「モデル診断および不確実性可視化ツール」へと昇華します。これははるかに強力な応用です。

第4章 ハイブリッド可視化の解釈：マップから洞察へ

4.1 MPPCA-SOMマップの読み解き方

ハイブリッドフレームワークによって生成されたマップは、従来のSOMやクラスタリング結果よりもはるかにリッチな情報を含んでいます。その解釈には、多角的な視点が必要です。



コアクラスターの特定: マップ上で、ある一つの成分平面が非常に高い確率（例：>0.9）を示し、他のすべての成分平面が低い値を示す領域を探します。これらは各クラスターの「核」となる、典型的な領域です。

クラスター境界の特定: 境界は、もはやU-Matrix上の「山」だけではありません。事後確率マップにおいて、境界とは、一つの成分平面から別の成分平面へと優位性が移り変わる「勾配」領域として現れます。

曖昧な境界と明確な境界: 2つの支配的な成分平面間の移行が急峻で狭い場合、それは明確でよく分離された境界を示します。一方、移行ゾーンが広く緩やかである場合、それはクラスター間の曖昧で重複した境界を示唆します。

クラスター間の関係性: SOMのトポロジーそのものが、クラスター間の関係性を明らかにします。もしクラスター1とクラスター3の領域がマップ上で隣接し、その間に移行ゾーンが存在する一方で、クラスター2が遠く離れている場合、それはクラスター1と3が、クラスター2よりも互いに類似していることを示唆します。

4.2 事後確率マップ：モデルの信頼度への視覚的ガイド

このフレームワークの真価は、モデルの不確実性を直接的に可視化できる点にあります。



曖昧性の特定: エントロピーマップを利用するか、複数の成分平面が中程度の確率（例：0.4〜0.6）を示す領域を調べることで、曖昧な領域を特定できます。これらのニューロンに写像されるデータ点は、MPPCAモデルが分類を困難と感じている点です [40, 41]。これは、モデルの不確実性に関する直接的で視覚的なフィードバックです。

外れ値／異常の特定: ヒット数が少なく、奇妙な確率プロファイルを持つ孤立したニューロンに写像されるデータ点は、外れ値である可能性があります。MPPCAモデルはこれらの点に対してすべての成分で低い尤度を割り当て、SOMはそれらをトポロジー的に孤立させます。

モデル適合度の評価: マップの大部分が低いエントロピー（高い信頼度）を示す場合、クラスターがよく分離されており、良好なモデル適合を示唆します。一方、高いエントロピーを持つ領域が広範囲にわたって存在する場合、選択したクラスター数（K）が不適切であるか、データ構造が本質的に複雑で重複している可能性を示唆します。

4.3 ケーススタディ分析：ウォークスルー

3つのクラスターを持つデータセットの可視化を例に、解釈のプロセスを具体的に見てみましょう。



まず、「勝者総取りマップ」を見て、3つの主要なクラスターの「領土」を大まかに把握します。

次に、3つの「成分平面」（p(c=1∣x), p(c=2∣x), p(c=3∣x) のヒートマップ）を個別に確認します。

最後に、「エントロピーマップ」でモデルの不確実性分布を調べます。

この一連のマップから、以下のような洞察が得られるかもしれません。「ここにクラスター1の核が見られます。クラスター2との境界は、成分平面の急峻な勾配として現れており、非常に明確です。しかし、クラスター1と3の間の境界ははるかに曖昧で、マップ上には広範な高エントロピー領域が存在します。この領域に写像されるデータ点の事後確率は、例えば [0.5,0.05,0.45] のようになっています。これは、モデルがこれらの点をタイプ1とタイプ3の混合と見なしていることを示しており、潜在的なサブクラスの存在や、これら2つのグループ間の連続的な移行を示唆している可能性があります。」

MPPCA-SOM可視化フレームワークは、データ点をその分類の確実性に基づいて「メタクラスタリング」することを可能にし、クラスタリングそのものよりも深い層の構造を明らかにします。



標準的なクラスタリングは、「この点はどのグループに属するか？」という問いに答えます。

MPPCA-SOMは、その問いに答えつつ、さらに「モデルはその割り当てにどれだけ確信を持っているか？そして、その確信度にはパターンがあるか？」という問いにも答えることができます。

事後確率空間をマッピングすることで、SOMは類似した「確信度プロファイル」を持つ点を自然にグループ化します [32]。

これにより、「高信頼度グループAの点」のクラスター、「高信頼度グループBの点」のクラスター、そして「AとBの間の低信頼度の点」のクラスターといった「メタクラスター」が形成されます。

これは二次的な洞察です。一次的な洞察はクラスタリングそのものです。二次的な洞察は、そのクラスタリングに関するモデルの信頼度の構造です。分析者は、これらの曖昧な点を具体的に調査することで、モデルがなぜ苦労しているのかを理解し、ノイズの多いラベル、データ入力エラー、あるいは測定対象のシステムが本質的に持つハイブリッドな状態などを発見する可能性があります。

第5章 比較分析と文脈化

MPPCA-SOMハイブリッドフレームワークの価値を正しく評価するためには、他の主要な次元削減・可視化手法との比較が不可欠です。

5.1 他の可視化手法との比較

PCA、t-SNE、UMAP

PCA vs. MPPCA-SOM: 古典的なPCAは線形な射影であり、データが単一のガウス分布に従う場合に最適です。MPPCA-SOMは、複数の局所的な線形モデルを組み合わせることで、非線形な構造を持つデータや、複数のクラスターが存在するデータをより柔軟に扱うことができます。また、PCAは確率的な解釈を提供しませんが、MPPCA-SOMはモデルの不確実性を可視化できます [9]。

t-SNE/UMAP vs. MPPCA-SOM: t-SNE（t-distributed Stochastic Neighbor Embedding）とUMAP（Uniform Manifold Approximation and Projection）は、高次元空間におけるデータの局所的な近傍構造を低次元空間で保持することに長けた、強力な非線形可視化手法です [42]。これらはしばしば、視覚的に魅力的なクラスター分離を示します。しかし、いくつかの重要な違いがあります。

大域的構造: t-SNEは、クラスター間の距離や相対的な配置といった大域的な構造を保持することが保証されておらず、しばしば歪めます。UMAPはt-SNEよりも大域的構造を保持すると主張されていますが、これには議論の余地があります [43]。一方、SOMはトポロジーを保持するように設計されており、MPPCA-SOMはこのトポロジー構造に確率的な意味付けを与えます。

確率的解釈: t-SNEとUMAPは、確率的な類似度に基づいていますが、MPPCA-SOMのようにデータ点に対する明確な生成モデルや事後確率を提供するわけではありません。そのため、モデルの信頼度や曖昧性を直接可視化する能力に欠けます。

目的: t-SNEとUMAPは主に「可視化」を目的としたアルゴリズムですが、MPPCA-SOMは「モデルベースのクラスタリング」と「可視化」を統合したフレームワークであり、より深いモデル解釈と診断を可能にします。

GTM（生成的トポグラフィックマッピング）

GTMの概要: GTM（Generative Topographic Mapping）は、SOMの直接的な確率的代替手法として開発されました [30, 34]。SOMと同様にトポロジーを保持するマップを生成しますが、明確な確率モデルと尤度関数に基づいているため、SOMの持つ理論的な欠点（コスト関数の不在、収束性の問題など）の多くを克服しています [31]。

GTM vs. MPPCA-SOM:

アプローチ: GTMは、潜在空間からデータ空間への非線形写像を学習する単一の生成モデルです。一方、MPPCA-SOMは、まずデータ空間を局所的な線形モデルの混合（MPPCA）で捉え、その結果（事後確率）をトポロジー保持マップ（SOM）で可視化するという、二段階のハイブリッドアプローチです。

柔軟性: MPPCA-SOMは、各クラスターが異なる次元の部分空間を持つことを許容するなど、より柔軟な局所構造のモデル化が可能です。GTMは単一の滑らかな多様体を仮定します。

解釈性: MPPCA-SOMは、「どのクラスターに属するか」という確率を直接的に可視化するため、解釈が非常に直感的です。GTMも事後確率（責任）を計算しますが、その解釈は潜在空間上の位置に対応します。どちらのアプローチも principled ですが、解釈の焦点が異なります。

VAE（変分オートエンコーダ）ハイブリッド

深層学習との融合: 近年、変分オートエンコーダ（VAE）のような深層生成モデルとクラスタリングやSOMのような構造を組み合わせる研究が活発化しています [44, 45]。

MoE-VAE (Mixture-of-Experts VAE): VAEのデコーダ部分を複数の専門家ネットワーク（エキスパート）の混合にすることで、MPPCAと同様にデータのマルチモーダルな分布を学習します。これは、MPPCAのアイデアを深層学習の文脈で実現したものと見なせます [46, 47]。

DPSOM (Deep Probabilistic Clustering with SOM): VAEを用いてデータの表現学習を行い、その潜在空間上で確率的なSOMを構築するモデルです。これは、MPPCA-SOMのフレームワークを、特徴抽出器として深層ニューラルネットワークを用いることで拡張したものと考えることができます [48]。

これらの深層学習ベースの手法は、画像などの非常に複雑なデータに対して高い表現力を持ちますが、モデルの複雑性や訓練の難しさが増大するというトレードオフがあります。

表2：主要な可視化手法の比較

特徴PCAt-SNE / UMAPGTMMPPCA-SOMモデルのタイプ線形射影非線形近傍グラフ確率的生成モデルハイブリッド（混合モデル＋NN）確率的基盤なしなし（類似度は確率的）あり（尤度関数）あり（尤度関数、事後確率）トポロジー保持なし局所的（大域的構造は歪む可能性）あり（滑らかな多様体）あり（離散グリッド）不確実性可視化不可不可間接的（潜在空間の分布）直接的（事後確率マップ）主な長所高速、解釈が容易優れたクラスター分離（視覚的）理論的厳密性、収束保証モデル解釈性、不確実性の可視化主な短所線形性のみ大域的構造の歪み、パラメータ感度柔軟性の制約二段階のモデル構築、計算コスト

Google スプレッドシートにエクスポート

第6章 応用、課題、そして今後の展望

6.1 応用分野

MPPCAとSOM、およびそれらの組み合わせは、多様な分野でその有効性が示されています。このハイブリッドフレームワークは、特に以下のような分野で強力なツールとなり得ます。



パターン認識: データに内在する複雑なパターンを、クラスター構造とその間の関係性として可視化することは、パターン認識の核心的なタスクです [49, 50]。MPPCA-SOMは、単にパターンを分類するだけでなく、パターン間の曖昧さや移行領域を特定することで、より深い理解を可能にします。例えば、環境モニタリングにおいて、電子鼻センサーのデータから異なる大気組成のパターンを識別し、それらのパターンがどの程度明確に分離されているかを評価するのに役立ちます [51]。

バイオインフォマティクス: 遺伝子発現データや細胞計測データ（サイトメトリー）など、バイオインフォマティクスにおけるデータは高次元かつノイズが多いという特徴があります [52, 53]。MPPCA-SOMは、異なる細胞集団をクラスタリングし、それらの集団間の分化経路や中間状態を、マップ上のトポロジーとして可視化する可能性があります。例えば、FlowSOMは、サイトメトリーデータ分析にSOMを応用した成功例であり [54]、MPPCAを組み合わせることで、各細胞が特定の細胞タイプに属する確率的な信頼度を可視化するという、さらなる次元の情報を加えることができます。

リモートセンシング: 衛星画像は、多数のスペクトルバンドを持つ高次元データです [55]。PCAやSOMは、土地被覆分類や変化検出などのタスクで広く利用されています [56, 57]。MPPCA-SOMフレームワークは、異なる土地被覆タイプ（例：森林、水域、都市部）をクラスタリングするだけでなく、それらの混合ピクセル（例：水辺の植生）を、マップ上で高い不確実性を持つ領域として特定するのに応用できます。これにより、より精緻な土地利用マップの作成や、生態系の移行帯の分析が可能になります。

6.2 実装上の課題

この強力なフレームワークを実用する際には、いくつかの課題を考慮する必要があります。



計算コスト: MPPCA、特にEMアルゴリズムによるパラメータ推定は、データセットのサイズと次元数、そして混合成分の数に対して計算コストが高くなる可能性があります。同様に、SOMの訓練も、特に大規模なデータセットや大きなマップサイズの場合、反復計算に時間を要します [33]。したがって、大規模データへの適用には、効率的な実装や、計算コストを削減するための戦略（例：粗いグリッドでの探索から始めるなど）が必要となります [58]。

ハイパーパラメータチューニング: このフレームワークの性能は、多くのハイパーパラメータに依存します。

MPPCA側: 混合成分数（K）、各成分の潜在次元数（q）。

SOM側: グリッドサイズ、トポロジー、学習率と近傍関数の減衰スケジュール。

これらのパラメータの最適な組み合わせを見つけるプロセス（ハイパーパラメータチューニング）は、それ自体が計算集約的な探索問題となります [59, 60]。グリッドサーチやランダムサーチ、より高度なベイズ最適化などの手法が考えられますが、評価関数の計算コストが高いことが課題となります [61, 62]。

6.3 将来の方向性

MPPCA-SOMフレームワークは、さらなる発展の可能性を秘めています。



動的・階層的モデル: 現実世界の多くのシステムは、時間とともに変化する動的な性質や、階層的な構造を持っています。SOMを動的あるいは階層的に拡張する研究（例：Growing Hierarchical Tree SOM）が存在します [63, 64]。MPPCAもまた階層的に構築可能です [3, 16]。これらの動的・階層的なアプローチを組み合わせることで、時系列データの状態遷移や、データの階層構造を、時間的・構造的なトポロジーとして可視化する、より高度な分析が可能になるでしょう。

深層学習とのさらなる統合: 第5章で触れたように、VAEとSOMの組み合わせは有望な方向性です [48]。MPPCAの局所線形モデルの混合というアイデアを、VAEの強力な非線形特徴抽出能力と組み合わせることで（例：MoE-VAE）、MPPCA-SOMフレームワークの原理を、より複雑で大規模なデータセット（特に画像や音声）へと拡張できます [44, 46]。SOMを、深層学習モデルの潜在空間を可視化し、解釈するための診断ツールとして活用する研究は、今後ますます重要になるでしょう。

確率的SOM（PSOM）の活用: SOM自体に確率的な要素を導入する研究も存在します。例えば、不確実性情報を重み更新ルールに直接組み込むProbabilistic SOM（PSOM）は、PPCAと階層的に組み合わせることで、より principled な方法で不確実性を扱うことができます [37]。このアプローチでは、事後確率をSOMへの入力とするのではなく、SOMの学習プロセス自体が確率的になります。これにより、最終的なマップの性能が向上する可能性が示唆されています。

結論

本レポートでは、混合確率的主成分分析（MPPCA）と自己組織化マップ（SOM）を統合したハイブリッドフレームワークについて、その理論的基盤から応用、解釈に至るまでを深く検討しました。この分析を通じて、以下の重要な結論が導き出されます。



相乗効果による価値の創出: MPPCAとSOMの組み合わせは、単なる逐次的な処理パイプラインではなく、真の相乗効果を生み出します。MPPCAが提供するモデルベースのクラスタリングと確率的な出力は、SOMが可視化するための入力として理想的です。一方、SOMが提供する直感的なトポロジー保持マップは、MPPCAの複雑な確率的出力を人間が解釈可能な形に変換します。

不確実性の可視化という新たなパラダイム: このフレームワークの最も革新的な側面は、データのクラスター構造だけでなく、モデル自身の「不確実性」を可視化できる点にあります。MPPCAの事後確率をSOMの入力とすることで、データ点がどのクラスターに属するかについてのモデルの「信念」や「曖昧さ」の分布を直接マップ上に表現できます。これにより、分析者はクラスターの核となる領域、明確な境界、そして曖昧な移行領域を特定でき、データとモデルの両方に対するより深い洞察を得ることが可能になります。

強力な探索的データ分析・モデル診断ツール: 結果として得られる「事後確率マップ」は、単なる可視化ツールを超え、強力な探索的データ分析およびモデル診断ツールとして機能します。マップ上の高不確実性領域は、データに内在する複雑性、潜在的なサブクラス、あるいはモデルの不適合を示唆する重要なシグナルです。これにより、分析者はデータ駆動的な仮説生成や、モデル改善のための具体的な指針を得ることができます。

文脈における位置づけ: t-SNEやUMAPのような現代的な可視化手法が視覚的なクラスター分離に優れる一方で、MPPCA-SOMはモデルの解釈性と確率的厳密性において独自の価値を提供します。また、GTMやVAEベースのハイブリッドモデルは、それぞれ異なるアプローチで同様の問題に取り組んでおり、MPPCA-SOMは、局所線形モデルの柔軟性と直感的な不確実性可視化という点で、これらの手法と相互補完的な関係にあります。

結論として、MPPCAとSOMの統合フレームワークは、高次元データに隠された構造を解き明かすための洗練されたアプローチです。それは、データが語る物語を「見る」だけでなく、その物語に対するモデルの「確信度」をも「見る」ことを可能にします。計算コストやハイパーパラメータチューニングといった課題は残るものの、このフレームワークが提供する深い洞察は、多くの科学的・商業的分野において、データ駆動型の意思決定を新たなレベルへと引き上げるポテンシャルを秘めています。
```

上記は私がこれから行う予定の研究です
自分の環境にプログラムを整理しています

データの内容は以下です

./prmsl_era5_all_data_seasonal_small.nc
```
(acs_cpu_env) root@a30a09056cae:/app# ncdump -h src/PressurePattern/SOM-MPPCA/prmsl_era5_all_data_seasonal_small.nc
netcdf prmsl_era5_all_data_seasonal_small {
dimensions:
        valid_time = 31047 ;
        latitude = 81 ;
        longitude = 81 ;
variables:
        float msl(valid_time, latitude, longitude) ;
                msl:_FillValue = NaNf ;
                msl:GRIB_paramId = 151LL ;
                msl:GRIB_dataType = "an" ;
                msl:GRIB_numberOfPoints = 6561LL ;
                msl:GRIB_typeOfLevel = "surface" ;
                msl:GRIB_stepUnits = 1LL ;
                msl:GRIB_stepType = "instant" ;
                msl:GRIB_gridType = "regular_ll" ;
                msl:GRIB_uvRelativeToGrid = 0LL ;
                msl:GRIB_NV = 0LL ;
                msl:GRIB_Nx = 81LL ;
                msl:GRIB_Ny = 81LL ;
                msl:GRIB_cfName = "air_pressure_at_mean_sea_level" ;
                msl:GRIB_cfVarName = "msl" ;
                msl:GRIB_gridDefinitionDescription = "Latitude/Longitude Grid" ;
                msl:GRIB_iDirectionIncrementInDegrees = 0.25 ;
                msl:GRIB_iScansNegatively = 0LL ;
                msl:GRIB_jDirectionIncrementInDegrees = 0.25 ;
                msl:GRIB_jPointsAreConsecutive = 0LL ;
                msl:GRIB_jScansPositively = 0LL ;
                msl:GRIB_latitudeOfFirstGridPointInDegrees = 45. ;
                msl:GRIB_latitudeOfLastGridPointInDegrees = 25. ;
                msl:GRIB_longitudeOfFirstGridPointInDegrees = 125. ;
                msl:GRIB_longitudeOfLastGridPointInDegrees = 145. ;
                msl:GRIB_missingValue = 3.40282346638529e+38 ;
                msl:GRIB_name = "Mean sea level pressure" ;
                msl:GRIB_shortName = "msl" ;
                msl:GRIB_totalNumber = 0LL ;
                msl:GRIB_units = "Pa" ;
                msl:long_name = "Mean sea level pressure" ;
                msl:units = "Pa" ;
                msl:standard_name = "air_pressure_at_mean_sea_level" ;
                msl:GRIB_surface = 0. ;
                msl:coordinates = "number valid_time latitude longitude expver" ;
        int64 number ;
                number:long_name = "ensemble member numerical id" ;
                number:units = "1" ;
                number:standard_name = "realization" ;
        int64 valid_time(valid_time) ;
                valid_time:long_name = "time" ;
                valid_time:standard_name = "time" ;
                valid_time:units = "seconds since 1970-01-01" ;
                valid_time:calendar = "proleptic_gregorian" ;
        double latitude(latitude) ;
                latitude:_FillValue = NaN ;
                latitude:units = "degrees_north" ;
                latitude:standard_name = "latitude" ;
                latitude:long_name = "latitude" ;
                latitude:stored_direction = "decreasing" ;
        double longitude(longitude) ;
                longitude:_FillValue = NaN ;
                longitude:units = "degrees_east" ;
                longitude:standard_name = "longitude" ;
                longitude:long_name = "longitude" ;
        string expver(valid_time) ;
        double f1_season(valid_time) ;
                f1_season:_FillValue = NaN ;
                f1_season:long_name = "Seasonal feature f1 (winter/summer contrast)" ;
                f1_season:description = "cos(2*pi*t) where t is normalized time from winter solstice to the next. Winter=+1, Summer=-1." ;
                f1_season:coordinates = "expver number" ;
        double f2_season(valid_time) ;
                f2_season:_FillValue = NaN ;
                f2_season:long_name = "Seasonal feature f2 (equinox grouping)" ;
                f2_season:description = "cos(4*pi*t) where t is normalized time from winter solstice to the next. Solstices=+1, Equinoxes=-1." ;
                f2_season:coordinates = "expver number" ;
        string label(valid_time) ;
                label:coordinates = "expver number" ;

// global attributes:
                :GRIB_centre = "ecmf" ;
                :GRIB_centreDescription = "European Centre for Medium-Range Weather Forecasts" ;
                :GRIB_subCentre = 0LL ;
                :Conventions = "CF-1.7" ;
                :institution = "European Centre for Medium-Range Weather Forecasts" ;
                :history = "2025-07-24T07:51 GRIB to CDM+CF via cfgrib-0.9.15.0/ecCodes-2.41.0 with {\"source\": \"tmpnp77bc4y/data.grib\", \"filter_by_keys\": {\"stream\": [\"oper\"], \"stepType\": [\"instant\"]}, \"encode_cf\": [\"parameter\", \"time\", \"geography\", \"vertical\"]}" ;
}
```
f1_seasonとf2_seasonは今回は使わないので無視してください

また、必要な補助プログラムは以下です

./minisom.py
```
from numpy import (array, unravel_index, nditer, linalg, random, subtract, max,
                   power, exp, zeros, ones, arange, outer, meshgrid, dot,
                   logical_and, mean, cov, argsort, linspace,
                   einsum, prod, nan, sqrt, hstack, diff, argmin, multiply,
                   nanmean, nansum, tile, array_equal, isclose, bincount)
from numpy.linalg import norm
from collections import defaultdict, Counter
from warnings import warn
from sys import stdout
from time import time
from datetime import timedelta
import pickle
import os
# Numbaをインポート
import numba
from numba import jit

"""
    Minimalistic implementation of the Self Organizing Maps (SOM).
"""


def _build_iteration_indexes(data_len, num_iterations,
                             verbose=False, random_generator=None,
                             use_epochs=False):
    """Returns an iterable with the indexes of the samples
    to pick at each iteration of the training.

    If random_generator is not None, it must be an instance
    of numpy.random.RandomState and it will be used
    to randomize the order of the samples."""
    if use_epochs:
        iterations_per_epoch = arange(data_len)
        if random_generator:
            random_generator.shuffle(iterations_per_epoch)
        iterations = tile(iterations_per_epoch, num_iterations)
    else:
        iterations = arange(num_iterations) % data_len
        if random_generator:
            random_generator.shuffle(iterations)
    if verbose:
        return _wrap_index__in_verbose(iterations)
    else:
        return iterations


def _wrap_index__in_verbose(iterations):
    """Yields the values in iterations printing the status on the stdout."""
    m = len(iterations)
    digits = len(str(m))
    progress = '\r [ {s:{d}} / {m} ] {s:3.0f}% - ? it/s'
    progress = progress.format(m=m, d=digits, s=0)
    stdout.write(progress)
    beginning = time()
    stdout.write(progress)
    for i, it in enumerate(iterations):
        yield it
        sec_left = ((m-i+1) * (time() - beginning)) / (i+1)
        time_left = str(timedelta(seconds=sec_left))[:7]
        progress = '\r [ {i:{d}} / {m} ]'.format(i=i+1, d=digits, m=m)
        progress += ' {p:3.0f}%'.format(p=100*(i+1)/m)
        progress += ' - {time_left} left '.format(time_left=time_left)
        stdout.write(progress)


# NumbaのJITデコレータを追加して高速化
@jit(nopython=True, cache=True)
def fast_norm(x):
    """Returns norm-2 of a 1-D numpy array.
    """
    return sqrt(dot(x, x.T))


# Numba JITコンパイル用のヘルパー関数 (distance_mapのループ部分)
@jit(nopython=True, cache=True)
def _distance_map_loop(weights, um, ii, jj):
    """Numba-accelerated loop for distance_map."""
    for x in range(weights.shape[0]):
        for y in range(weights.shape[1]):
            w_2 = weights[x, y]
            e = y % 2 == 0  # only used on hexagonal topology
            for k, (i, j) in enumerate(zip(ii[e], jj[e])):
                if (x + i >= 0 and x + i < weights.shape[0] and
                        y + j >= 0 and y + j < weights.shape[1]):
                    w_1 = weights[x + i, y + j]
                    um[x, y, k] = fast_norm(w_2 - w_1)
    return um


class MiniSom(object):
    Y_HEX_CONV_FACTOR = (3.0 / 2.0) / sqrt(3)

    def __init__(self, x, y, input_len, sigma=1, learning_rate=0.5,
                 decay_function='asymptotic_decay',
                 neighborhood_function='gaussian', topology='rectangular',
                 activation_distance='euclidean', random_seed=None,
                 sigma_decay_function='asymptotic_decay'):
        """Initializes a Self Organizing Maps.

        A rule of thumb to set the size of the grid for a dimensionality
        reduction task is that it should contain 5*sqrt(N) neurons
        where N is the number of samples in the dataset to analyze.

        E.g. if your dataset has 150 samples, 5*sqrt(150) = 61.23
        hence a map 8-by-8 should perform well.

        Parameters
        ----------
        x : int
            x dimension of the SOM.

        y : int
            y dimension of the SOM.

        input_len : int
            Number of the elements of the vectors in input.

        sigma : float, optional (default=1)
            Spread of the neighborhood function.

            Needs to be adequate to the dimensions of the map
            and the neighborhood function. In some cases it
            helps to set sigma as sqrt(x^2 +y^2).

        learning_rate : float, optional (default=0.5)
            Initial learning rate.

            Adequate values are dependent on the data used for training.

            By default, at the iteration t, we have:
                learning_rate(t) = learning_rate / (1 + t * (100 / max_iter))

        decay_function : string or callable, optional
        (default='asymptotic_decay')
            Function that reduces learning_rate at each iteration.
            Possible values: 'inverse_decay_to_zero', 'linear_decay_to_zero',
                             'asymptotic_decay' or callable

            If a custom decay function using a callable
            it will need to to take in input
            three parameters in the following order:

            1. learning rate
            2. current iteration
            3. maximum number of iterations allowed

            Note that if a lambda function is used to define the decay
            MiniSom will not be pickable anymore.

        neighborhood_function : string, optional (default='gaussian')
            Function that weights the neighborhood of a position in the map.
            Possible values: 'gaussian', 'mexican_hat', 'bubble', 'triangle'

        topology : string, optional (default='rectangular')
            Topology of the map.
            Possible values: 'rectangular', 'hexagonal'

        activation_distance : string, callable optional (default='euclidean')
            Distance used to activate the map.
            Possible values: 'euclidean', 'cosine', 'manhattan', 'chebyshev'

            Example of callable that can be passed:

            def euclidean(x, w):
                return linalg.norm(subtract(x, w), axis=-1)

        random_seed : int, optional (default=None)
            Random seed to use.

        sigma_decay_function : string, optional
        (default='asymptotic_decay')
            Function that reduces sigma at each iteration.
            Possible values: 'inverse_decay_to_one', 'linear_decay_to_one',
                             'asymptotic_decay'

            The default function is:
                sigma(t) = sigma / (1 + (t * (sigma - 1) / max_iter))
        """
        if sigma > sqrt(x*x + y*y):
            warn('Warning: sigma might be too high ' +
                 'for the dimension of the map.')

        self._random_generator = random.RandomState(random_seed)

        self._learning_rate = learning_rate
        self._sigma = sigma
        self._input_len = input_len
        # random initialization
        self._weights = self._random_generator.rand(x, y, input_len)*2-1
        self._weights /= linalg.norm(self._weights, axis=-1, keepdims=True)

        self._activation_map = zeros((x, y))
        self._neigx = arange(x, dtype=float)
        self._neigy = arange(y, dtype=float)

        if topology not in ['hexagonal', 'rectangular']:
            msg = '%s not supported only hexagonal and rectangular available'
            raise ValueError(msg % topology)
        self.topology = topology
        self._xx, self._yy = meshgrid(self._neigx, self._neigy)
        self._xx = self._xx.astype(float)
        self._yy = self._yy.astype(float)
        if topology == 'hexagonal':
            self._xx[::-2] -= 0.5
            self._yy *= self.Y_HEX_CONV_FACTOR
            if neighborhood_function in ['triangle']:
                warn('triangle neighborhood function does not ' +
                     'take in account hexagonal topology')

        lr_decay_functions = {
            'inverse_decay_to_zero': self._inverse_decay_to_zero,
            'linear_decay_to_zero': self._linear_decay_to_zero,
            'asymptotic_decay': self._asymptotic_decay}

        if isinstance(decay_function, str):
            if decay_function not in lr_decay_functions:
                msg = '%s not supported. Functions available: %s'
                raise ValueError(msg % (decay_function,
                                        ', '.join(lr_decay_functions.keys())))

            self._learning_rate_decay_function = \
                lr_decay_functions[decay_function]
        elif callable(decay_function):
            self._learning_rate_decay_function = decay_function

        sig_decay_functions = {
            'inverse_decay_to_one': self._inverse_decay_to_one,
            'linear_decay_to_one': self._linear_decay_to_one,
            'asymptotic_decay': self._asymptotic_decay}

        if sigma_decay_function not in sig_decay_functions:
            msg = '%s not supported. Functions available: %s'
            raise ValueError(msg % (sigma_decay_function,
                                    ', '.join(sig_decay_functions.keys())))

        self._sigma_decay_function = sig_decay_functions[sigma_decay_function]

        neig_functions = {'gaussian': self._gaussian,
                          'mexican_hat': self._mexican_hat,
                          'bubble': self._bubble,
                          'triangle': self._triangle}

        if neighborhood_function not in neig_functions:
            msg = '%s not supported. Functions available: %s'
            raise ValueError(msg % (neighborhood_function,
                                    ', '.join(neig_functions.keys())))

        if neighborhood_function in ['triangle',
                                     'bubble'] and (divmod(sigma, 1)[1] != 0
                                                    or sigma < 1):
            warn('sigma should be an integer >=1 when triangle or bubble' +
                 'are used as neighborhood function')

        self.neighborhood = neig_functions[neighborhood_function]

        distance_functions = {'euclidean': self._euclidean_distance,
                              'cosine': self._cosine_distance,
                              'manhattan': self._manhattan_distance,
                              'chebyshev': self._chebyshev_distance}

        if isinstance(activation_distance, str):
            if activation_distance not in distance_functions:
                msg = '%s not supported. Distances available: %s'
                raise ValueError(msg % (activation_distance,
                                        ', '.join(distance_functions.keys())))

            self._activation_distance = distance_functions[activation_distance]
        elif callable(activation_distance):
            self._activation_distance = activation_distance

    def get_weights(self):
        """Returns the weights of the neural network."""
        return self._weights

    def get_euclidean_coordinates(self):
        """Returns the position of the neurons on an euclidean
        plane that reflects the chosen topology in two meshgrids xx and yy.
        Neuron with map coordinates (1, 4) has coordinate (xx[1, 4], yy[1, 4])
        in the euclidean plane.

        Only useful if the topology chosen is not rectangular.
        """
        return self._xx.T, self._yy.T

    def convert_map_to_euclidean(self, xy):
        """Converts map coordinates into euclidean coordinates
        that reflects the chosen topology.

        Only useful if the topology chosen is not rectangular.
        """
        return self._xx.T[xy], self._yy.T[xy]

    def _activate(self, x):
        """Updates matrix activation_map, in this matrix
           the element i,j is the response of the neuron i,j to x."""
        self._activation_map = self._activation_distance(x, self._weights)

    def activate(self, x):
        """Returns the activation map to x."""
        self._activate(x)
        return self._activation_map

    def _inverse_decay_to_zero(self, learning_rate, t, max_iter):
        """Decay function of the learning process that asymptotically
        approaches zero.
        """
        C = max_iter / 100.0
        return learning_rate * C / (C + t)

    def _linear_decay_to_zero(self, learning_rate, t, max_iter):
        """Decay function of the learning process that linearly
        decreases to zero.
        """
        return learning_rate * (1 - t / max_iter)

    def _inverse_decay_to_one(self, sigma, t, max_iter):
        """Decay function of sigma that asymptotically approaches one.
        """
        C = (sigma - 1) / max_iter
        return sigma / (1 + (t * C))

    def _linear_decay_to_one(self, sigma, t, max_iter):
        """Decay function of sigma that linearly decreases
        to one.
        """
        return sigma + (t * (1 - sigma) / max_iter)

    def _asymptotic_decay(self, dynamic_parameter, t, max_iter):
        """Decay function of the learning process
        and sigma that decays these values asymptotically to 1/3
        of their original values.
        """
        return dynamic_parameter / (1 + t / (max_iter / 2))

    def _gaussian(self, c, sigma):
        """Returns a Gaussian centered in c."""
        d = 2*sigma*sigma
        ax = exp(-power(self._xx-self._xx.T[c], 2)/d)
        ay = exp(-power(self._yy-self._yy.T[c], 2)/d)
        return (ax * ay).T  # the external product gives a matrix

    def _mexican_hat(self, c, sigma):
        """Mexican hat centered in c."""
        p = power(self._xx-self._xx.T[c], 2) + power(self._yy-self._yy.T[c], 2)
        d = 2*sigma*sigma
        return (exp(-p/d)*(1-2/d*p)).T

    def _bubble(self, c, sigma):
        """Constant function centered in c with spread sigma.
        sigma should be an odd value.
        """
        ax = logical_and(self._neigx > c[0]-sigma,
                         self._neigx < c[0]+sigma)
        ay = logical_and(self._neigy > c[1]-sigma,
                         self._neigy < c[1]+sigma)
        return outer(ax, ay)*1.

    def _triangle(self, c, sigma):
        """Triangular function centered in c with spread sigma."""
        triangle_x = (-abs(c[0] - self._neigx)) + sigma
        triangle_y = (-abs(c[1] - self._neigy)) + sigma
        triangle_x[triangle_x < 0] = 0.
        triangle_y[triangle_y < 0] = 0.
        return outer(triangle_x, triangle_y)

    def _cosine_distance(self, x, w):
        num = (w * x).sum(axis=2)
        denum = multiply(linalg.norm(w, axis=2), linalg.norm(x))
        return 1 - num / (denum+1e-8)

    def _euclidean_distance(self, x, w):
        return linalg.norm(subtract(x, w), axis=-1)

    def _manhattan_distance(self, x, w):
        return linalg.norm(subtract(x, w), ord=1, axis=-1)

    def _chebyshev_distance(self, x, w):
        return max(subtract(x, w), axis=-1)

    def _check_iteration_number(self, num_iteration):
        if num_iteration < 1:
            raise ValueError('num_iteration must be > 1')

    def _check_input_len(self, data):
        """Checks that the data in input is of the correct shape."""
        data_len = len(data[0])
        if self._input_len != data_len:
            msg = 'Received %d features, expected %d.' % (data_len,
                                                          self._input_len)
            raise ValueError(msg)

    def winner(self, x):
        """Computes the coordinates of the winning neuron for the sample x."""
        self._activate(x)
        return unravel_index(self._activation_map.argmin(),
                             self._activation_map.shape)

    def update(self, x, win, t, max_iteration):
        """Updates the weights of the neurons.

        Parameters
        ----------
        x : np.array
            Current pattern to learn.
        win : tuple
            Position of the winning neuron for x (array or tuple).
        t : int
            rate of decay for sigma and learning rate
        max_iteration : int
            If use_epochs is True:
                Number of epochs the SOM will be trained for
            If use_epochs is False:
                Maximum number of iterations (one iteration per sample).
        """
        eta = self._learning_rate_decay_function(self._learning_rate,
                                                 t, max_iteration)
        sig = self._sigma_decay_function(self._sigma, t, max_iteration)
        # improves the performances
        g = self.neighborhood(win, sig)*eta
        # w_new = eta * neighborhood_function * (x-w)
        self._weights += einsum('ij, ijk->ijk', g, x-self._weights)

    def quantization(self, data):
        """Assigns a code book (weights vector of the winning neuron)
        to each sample in data."""
        self._check_input_len(data)
        winners_coords = argmin(self._distance_from_weights(data), axis=1)
        return self._weights[unravel_index(winners_coords,
                                           self._weights.shape[:2])]

    def random_weights_init(self, data):
        """Initializes the weights of the SOM
        picking random samples from data."""
        self._check_input_len(data)
        # --- ベクトル化による改善 ---
        map_shape = self._weights.shape[:2]
        rand_indices = self._random_generator.randint(len(data),
                                                      size=map_shape)
        self._weights = array(data)[rand_indices]

    def pca_weights_init(self, data):
        """Initializes the weights to span the first two principal components.

        This initialization doesn't depend on random processes and
        makes the training process converge faster.

        It is strongly reccomended to normalize the data before initializing
        the weights and use the same normalization for the training data.
        """
        if self._input_len == 1:
            msg = 'The data needs at least 2 features for pca initialization'
            raise ValueError(msg)
        self._check_input_len(data)
        if len(self._neigx) == 1 or len(self._neigy) == 1:
            msg = 'PCA initialization inappropriate:' + \
                  'One of the dimensions of the map is 1.'
            warn(msg)
        
        # --- ベクトル化による改善 ---
        pc_length, pc_vecs = linalg.eigh(cov(data.T))
        pc_order = argsort(-pc_length)
        pc_vecs = pc_vecs[:, pc_order]
        
        mx, my = self._weights.shape[0], self._weights.shape[1]
        c1 = linspace(-1, 1, mx)[:, None]
        c2 = linspace(-1, 1, my)
        
        pc1 = data.mean(axis=0) + c1 * sqrt(pc_length[pc_order[0]]) * pc_vecs[:, 0]
        pc2 = c2 * sqrt(pc_length[pc_order[1]]) * pc_vecs[:, 1]
        
        # Broadcasting to create the weights
        self._weights = pc1[:, None, :] + pc2[None, :, :]


    def _check_fixed_points(self, fixed_points, data):
        for k in fixed_points.keys():
            if not isinstance(k, int):
                raise TypeError(f'fixed points indexes must ' +
                                'be integers.')
            if k >= len(data) or k < 0:
                raise ValueError(f'an index of a fixed point ' +
                                 'cannot be grater than len(data)' +
                                 ' or less than 0.')
            if fixed_points[k][0] >= self._weights.shape[0] or \
               fixed_points[k][1] >= self._weights.shape[1]:
                raise ValueError(f'coordinates for fixed point' +
                                 ' are out of boundaries.')
            if fixed_points[k][0] < 0 or \
               fixed_points[k][1] < 0:
                raise ValueError(f'coordinates cannot be negative.')

    def train(self, data, num_iteration,
              random_order=False, verbose=False,
              use_epochs=False, fixed_points=None):
        """Trains the SOM.

        Parameters
        ----------
        data : np.array or list
            Data matrix.

        num_iteration : int
            If use_epochs is False, the weights will be
            updated num_iteration times. Otherwise they will be updated
            len(data)*num_iteration times.

        random_order : bool (default=False)
            If True, samples are picked in random order.
            Otherwise the samples are picked sequentially.

        verbose : bool (default=False)
            If True the status of the training will be
            printed each time the weights are updated.

        use_epochs : bool (default=False)
            If True the SOM will be trained for num_iteration epochs.
            In one epoch the weights are updated len(data) times and
            the learning rate is constat throughout a single epoch.

        fixed_points : dict (default=None)
            A dictionary k : (c_1, c_2), that will force the
            training algorithm to use the neuron with coordinates
            (c_1, c_2) as winner for the sample k instead of
            the best matching unit.
        """
        self._check_iteration_number(num_iteration)
        # We use array() here to make sure data is a numpy array
        data = array(data)
        self._check_input_len(data)
        random_generator = None
        if random_order:
            random_generator = self._random_generator
        iterations = _build_iteration_indexes(len(data), num_iteration,
                                              verbose, random_generator,
                                              use_epochs)
        if use_epochs:
            def get_decay_rate(iteration_index, data_len):
                return int(iteration_index / data_len)
        else:
            def get_decay_rate(iteration_index, data_len):
                return int(iteration_index)

        if fixed_points:
            self._check_fixed_points(fixed_points, data)
        else:
            fixed_points = {}

        for t, iteration in enumerate(iterations):
            decay_rate = get_decay_rate(t, len(data))
            self.update(data[iteration],
                        fixed_points.get(iteration,
                                         self.winner(data[iteration])),
                        decay_rate, num_iteration)
        if verbose:
            print('\n quantization error:', self.quantization_error(data))

    def train_random(self, data, num_iteration, verbose=False):
        """Trains the SOM picking samples at random from data.

        Parameters
        ----------
        data : np.array or list
            Data matrix.

        num_iteration : int
            Maximum number of iterations (one iteration per sample).

        verbose : bool (default=False)
            If True the status of the training
            will be printed at each time the weights are updated.
        """
        self.train(data, num_iteration, random_order=True, verbose=verbose)

    def train_batch(self, data, num_iteration, verbose=False):
        """Trains the SOM using all the vectors in data sequentially.

        Parameters
        ----------
        data : np.array or list
            Data matrix.

        num_iteration : int
            Maximum number of iterations (one iteration per sample).

        verbose : bool (default=False)
            If True the status of the training
            will be printed at each time the weights are updated.
        """
        self.train(data, num_iteration, random_order=False, verbose=verbose)

    def distance_map(self, scaling='sum'):
        """Returns the distance map of the weights.
        If scaling is 'sum' (default), each cell is the normalised sum of
        the distances between a neuron and its neighbours. Note that this
        method uses the euclidean distance.

        Parameters
        ----------
        scaling : string (default='sum')
            If set to 'mean', each cell will be the normalized
            by the average of the distances of the neighbours.
            If set to 'sum', the normalization is done
            by the sum of the distances.
        """
        if scaling not in ['sum', 'mean']:
            raise ValueError(f'scaling should be either "sum" or "mean" ('
                             f'"{scaling}" not valid)')
        
        # --- Numbaによるループ高速化 ---
        weights = self._weights
        um = nan * zeros((weights.shape[0], weights.shape[1], 8))

        if self.topology == 'hexagonal':
            # Numbaで扱えるようにarrayに変換
            ii = array([[1, 1, 1, 0, -1, 0], [0, 1, 0, -1, -1, -1]])
            jj = array([[1, 0, -1, -1, 0, 1], [1, 0, -1, -1, 0, 1]])
        else: # rectangular
            ii = array([[0, -1, -1, -1, 0, 1, 1, 1]]*2)
            jj = array([[-1, -1, 0, 1, 1, 1, 0, -1]]*2)

        um = _distance_map_loop(weights, um, ii, jj)
        
        if scaling == 'mean':
            um = nanmean(um, axis=2)
        if scaling == 'sum':
            um = nansum(um, axis=2)
            
        return um / um.max()

    def activation_response(self, data):
        """
            Returns a matrix where the element i,j is the number of times
            that the neuron i,j have been winner.
        """
        self._check_input_len(data)
        # --- ベクトル化による改善 ---
        map_shape = self._weights.shape[:2]
        # 全データに対して一括で勝者ニューロンを計算
        winners_flat = self._distance_from_weights(data).argmin(axis=1)
        # bincountで各ニューロンが勝者になった回数を集計
        win_counts = bincount(winners_flat, minlength=prod(map_shape))
        return win_counts.reshape(map_shape)

    def _distance_from_weights(self, data):
        """Returns a matrix d where d[i,j] is the euclidean distance between
        data[i] and the j-th weight.
        """
        input_data = array(data)
        weights_flat = self._weights.reshape(-1, self._weights.shape[2])
        input_data_sq = power(input_data, 2).sum(axis=1, keepdims=True)
        weights_flat_sq = power(weights_flat, 2).sum(axis=1, keepdims=True)
        cross_term = dot(input_data, weights_flat.T)
        return sqrt(-2 * cross_term + input_data_sq + weights_flat_sq.T)

    def quantization_error(self, data):
        """Returns the quantization error computed as the average
        distance between each input sample and its best matching unit."""
        self._check_input_len(data)
        return norm(data-self.quantization(data), axis=1).mean()

    def distortion_measure(self, data):
        """Returns the distortion measure computed as
           sum_i, sum_c (neighborhood(c, sigma) * || d_i - w_c ||^2
        """
        distortion = 0
        # NOTE: このループは各データ点で異なる勝者ニューロンと近傍関数を使うため、
        #       単純なベクトル化が困難です。Numba化も可能ですが、
        #       neighborhood関数がPythonオブジェクトであるためnopythonモードが使えず、
        #       効果が限定的です。元の実装を維持します。
        for d in data:
            winner_coords = self.winner(d)
            neighborhood_grid = self.neighborhood(winner_coords, self._sigma)
            distances = norm(d - self.get_weights(), axis=2)
            distortion += (neighborhood_grid * distances).sum()
        return distortion

    def topographic_error(self, data):
        """Returns the topographic error computed by finding
        the best-matching and second-best-matching neuron in the map
        for each input and then evaluating the positions.

        A sample for which these two nodes are not adjacent counts as
        an error. The topographic error is given by the
        the total number of errors divided by the total of samples.

        If the topographic error is 0, no error occurred.
        If 1, the topology was not preserved for any of the samples."""
        self._check_input_len(data)
        total_neurons = prod(self._activation_map.shape)
        if total_neurons <= 1:
            warn('The topographic error is not defined for a map with less than 2 neurons.')
            return nan
        
        # --- ベクトル化による改善 ---
        # best 2 matching units
        b2mu_inds = self._distance_from_weights(data).argpartition(2, axis=1)[:, :2]
        
        if self.topology == 'hexagonal':
            b2mu_coords = self._get_euclidean_coordinates_from_index(b2mu_inds)
            # norm of the distance between the two winners for each data point
            bmu_diff = norm(b2mu_coords[:, 0, :] - b2mu_coords[:, 1, :], axis=1)
            # isclose(1, ...) is used to check for adjacency in hex grid
            return (1 - isclose(bmu_diff, 1.0)).mean()
        else: # rectangular
            b2mu_x, b2mu_y = unravel_index(b2mu_inds, self._weights.shape[:2])
            # distance is > 1.42 (approx. sqrt(2)) if not adjacent
            dx = diff(b2mu_x, axis=1)
            dy = diff(b2mu_y, axis=1)
            distance = sqrt(dx**2 + dy**2).flatten()
            return (distance > 1.42).mean()

    def _get_euclidean_coordinates_from_index(self, index_array):
        """Returns the Euclidean coordinated of neurons from an array of indices."""
        y_dim = self._weights.shape[1]
        coords_x = (index_array // y_dim).astype(int)
        coords_y = (index_array % y_dim).astype(int)
        
        # Use advanced indexing on the meshgrids
        euclidean_x = self._xx.T[coords_x, coords_y]
        euclidean_y = self._yy.T[coords_x, coords_y]
        
        # Stack to get shape (n_samples, n_bmu, 2)
        return hstack([euclidean_x[..., None], euclidean_y[..., None]]).reshape(index_array.shape[0], index_array.shape[1], 2)

    def win_map(self, data, return_indices=False):
        """Returns a dictionary wm where wm[(i,j)] is a list with:
        - all the patterns that have been mapped to the position (i,j),
          if return_indices=False (default)
        - all indices of the elements that have been mapped to the
          position (i,j) if return_indices=True"""
        self._check_input_len(data)
        winmap = defaultdict(list)
        # --- ベクトル化による改善 ---
        # 最初に全データの勝者を一括計算
        winners = self._winners_from_weights(data)
        data_to_append = arange(len(data)) if return_indices else data

        for i, win_pos in enumerate(winners):
            winmap[win_pos].append(data_to_append[i])
        return winmap

    def labels_map(self, data, labels):
        """Returns a dictionary wm where wm[(i,j)] is a dictionary
        that contains the number of samples from a given label
        that have been mapped in position i,j.

        Parameters
        ----------
        data : np.array or list
            Data matrix.

        label : np.array or list
            Labels for each sample in data.
        """
        self._check_input_len(data)
        if not len(data) == len(labels):
            raise ValueError('data and labels must have the same length.')
        winmap = defaultdict(list)
        # --- ベクトル化による改善 ---
        # 最初に全データの勝者を一括計算
        winners = self._winners_from_weights(data)
        for win_pos, label in zip(winners, labels):
            winmap[win_pos].append(label)

        for position in winmap:
            winmap[position] = Counter(winmap[position])
        return winmap
        
    def _winners_from_weights(self, data):
        """Helper function to return the winner coordinates for all data points."""
        distances = self._distance_from_weights(data)
        winner_indices = argmin(distances, axis=1)
        return [unravel_index(i, self._weights.shape[:2]) for i in winner_indices]
```

./mppca.py
```
# Translation in python of the Matlab implementation of Mathieu Andreux and
# Michel Blancard, of the algorithm described in
# "Mixtures of Probabilistic Principal Component Analysers",
# Michael E. Tipping and Christopher M. Bishop, Neural Computation 11(2),
# pp 443–482, MIT Press, 1999

import numpy as np
import numba

# Numbaデコレータを追加して関数をJITコンパイルする
# 注意: variance_levelを使用する場合、Noneではなく数値を渡してください
@numba.jit(nopython=True)
def initialization_kmeans(X, p, q, variance_level):
    """
    X : dataset
    p : number of clusters
    q : dimension of the latent space
    variance_level

    pi : proportions of clusters
    mu : centers of the clusters in the observation space
    W : latent to observation matricies
    sigma2 : noise
    """

    N, d = X.shape

    # initialization
    # Numbaはnp.random.seedをサポートしていないため、外部で設定が必要
    init_centers = np.random.randint(0, N, p)
    # uniqueな中心が選ばれるまで繰り返すロジックは同じ
    while (len(np.unique(init_centers)) != p):
        init_centers = np.random.randint(0, N, p)

    mu = X[init_centers, :]
    distance_square = np.zeros((N, p))
    clusters = np.zeros(N, dtype=np.int32)

    D_old = -2.0
    D = -1.0

    while(D_old != D):
        D_old = D

        # assign clusters
        for c in range(p):
            # distance_square[:, c] = np.power(X - mu[c, :], 2).sum(1)
            # 上記の処理をNumbaが最適化しやすいように明示的なループで記述
            for i in range(N):
                sum_sq = 0.0
                for j in range(d):
                    diff = X[i, j] - mu[c, j]
                    sum_sq += diff * diff
                distance_square[i, c] = sum_sq

        for i in range(N):
            clusters[i] = np.argmin(distance_square[i,:])


        # compute distortion
        dist_sum = 0.0
        for i in range(N):
            dist_sum += distance_square[i, clusters[i]]
        D = dist_sum
        
        distmin = np.zeros(N)
        for i in range(N):
            distmin[i] = distance_square[i, clusters[i]]

        # compute new centers
        # 元のコード `mu[c, :] = X[clusters == c, :].mean(0)` をNumbaフレンドリーに書き換え
        sum_mu = np.zeros((p, d))
        counts = np.zeros(p)
        for i in range(N):
            c_idx = clusters[i]
            sum_mu[c_idx, :] += X[i, :]
            counts[c_idx] += 1
        
        for c in range(p):
            if counts[c] > 0:
                mu[c, :] = sum_mu[c, :] / counts[c]
            # 空のクラスタのケースは元のコードも未定義なため、ここでは何もしない

    # parameter initialization
    pi = np.zeros(p)
    W = np.zeros((p, d, q))
    sigma2 = np.zeros(p)

    # 元の初期化ループをNumbaフレンドリーに書き換え
    # まずクラスタごとの統計量を計算
    cluster_counts = np.zeros(p)
    distmin_sum = np.zeros(p)
    for i in range(N):
        c_idx = clusters[i]
        cluster_counts[c_idx] += 1
        distmin_sum[c_idx] += distmin[i]

    for c in range(p):
        if variance_level != -1.0: # variance_levelが指定されているか（Noneの代わりに-1.0等で判定）
            W[c, :, :] = variance_level * np.random.randn(d, q)
            sigma2[c] = np.abs((variance_level/10) * np.random.randn())
        else:
            W[c, :, :] = np.random.randn(d, q)
            if cluster_counts[c] > 0:
                sigma2[c] = (distmin_sum[c] / cluster_counts[c]) / d
            else:
                sigma2[c] = 1.0 # 空クラスタの場合のデフォルト値

        pi[c] = cluster_counts[c] / N

    return pi, mu, W, sigma2, clusters


# Numbaデコレータを追加. fastmath=Trueでさらに高速化
@numba.jit(nopython=True, fastmath=True)
def mppca_gem(X, pi, mu, W, sigma2, niter):
    N, d = X.shape
    p = len(sigma2)
    _, q = W[0].shape

    sigma2hist = np.zeros((p, niter))
    M = np.zeros((p, q, q))
    Minv = np.zeros((p, q, q))
    # Cinvはサイズが大きいためループ内で生成
    logR = np.zeros((N, p))
    R = np.zeros((N, p))

    L = np.zeros(niter)
    # print文はNumbaのnopythonモードではサポートされないため削除
    for i in range(niter):
        for c in range(p):
            sigma2hist[c, i] = sigma2[c]

            # M
            M_c = sigma2[c]*np.eye(q) + W[c, :, :].T @ W[c, :, :]
            M[c, :, :] = M_c
            Minv[c, :, :] = np.linalg.inv(M_c)

            # Cinv
            W_c = W[c, :, :]
            Minv_c = Minv[c, :, :]
            Cinv_c = (np.eye(d) - W_c @ Minv_c @ W_c.T) / sigma2[c]
            
            # R_ni
            # 改善点: np.log(np.linalg.det) を np.linalg.slogdet に変更
            inner_mat = np.eye(d) - W_c @ Minv_c @ W_c.T
            sign, logdet = np.linalg.slogdet(inner_mat)
            log_det_term = logdet # signは+1と仮定

            deviation_from_center = X - mu[c, :]
            
            # (dev * (dev @ Cinv.T)).sum(1) を効率的に計算
            quad_term = np.zeros(N)
            for k in range(N):
                quad_term[k] = deviation_from_center[k,:] @ Cinv_c @ deviation_from_center[k,:].T

            logR[:, c] = ( np.log(pi[c])
                + 0.5 * log_det_term
                - 0.5 * d * np.log(sigma2[c])
                - 0.5 * quad_term
                )

        # myMaxの計算とlog-sum-expトリック
        myMax = np.zeros(N)
        for k in range(N):
            myMax[k] = np.max(logR[k, :])
        
        log_sum_exp = np.zeros(N)
        for k in range(N):
            log_sum_exp[k] = myMax[k] + np.log(np.sum(np.exp(logR[k, :] - myMax[k])))
        
        L[i] = np.sum(log_sum_exp) - N*d*np.log(2*3.141593)/2.
        
        # logRの正規化
        for k in range(N):
            logR[k, :] = logR[k, :] - log_sum_exp[k]
        
        # piの更新
        log_pi_sum_exp = np.zeros(p)
        myMax_pi = np.zeros(p)
        for c in range(p):
            myMax_pi[c] = np.max(logR[:, c])

        for c in range(p):
            sum_val = 0.0
            for k in range(N):
                sum_val += np.exp(logR[k, c] - myMax_pi[c])
            log_pi_sum_exp[c] = np.log(sum_val)

        logpi = myMax_pi + log_pi_sum_exp - np.log(N)
        pi = np.exp(logpi)
        R = np.exp(logR)
        
        for c in range(p):
            R_c_sum = np.sum(R[:, c])
            mu[c, :] = (R[:, c].reshape(N, 1) * X).sum(axis=0) / R_c_sum
            
            deviation_from_center = X - mu[c, :].reshape(1, d)
            
            SW_numerator = (R[:, c].reshape(N, 1) * deviation_from_center).T @ (deviation_from_center @ W[c,:,:])
            SW = (1 / (pi[c]*N)) * SW_numerator
            
            Wnew = SW @ np.linalg.inv(sigma2[c]*np.eye(q) + Minv[c, :, :] @ W[c, :, :].T @ SW)
            
            term1_num = np.sum(R[:, c].reshape(N, 1) * np.power(deviation_from_center, 2))
            sigma2[c] = (1/d) * ( term1_num / (N*pi[c]) - np.trace(SW @ Minv[c, :, :] @ Wnew.T))

            W[c, :, :] = Wnew

    return pi, mu, W, sigma2, R, L, sigma2hist


# Numbaデコレータを追加
@numba.jit(nopython=True, fastmath=True)
def mppca_predict(X, pi, mu, W, sigma2):
    N, d = X.shape
    p = len(sigma2)
    _, q = W[0].shape

    M = np.zeros((p, q, q))
    Minv = np.zeros((p, q, q))
    logR = np.zeros((N, p))
    R = np.zeros((N, p))

    for c in range(p):
        # M
        M_c = sigma2[c] * np.eye(q) + W[c, :, :].T @ W[c, :, :]
        Minv_c = np.linalg.inv(M_c)

        # Cinv
        W_c = W[c, :, :]
        Cinv_c = (np.eye(d) - W_c @ Minv_c @ W_c.T) / sigma2[c]

        # R_ni
        # 改善点: np.log(np.linalg.det) を np.linalg.slogdet に変更
        inner_mat = np.eye(d) - W_c @ Minv_c @ W_c.T
        sign, logdet = np.linalg.slogdet(inner_mat)
        log_det_term = logdet # signは+1と仮定
        
        deviation_from_center = X - mu[c, :]
        
        quad_term = np.zeros(N)
        for k in range(N):
            quad_term[k] = deviation_from_center[k,:] @ Cinv_c @ deviation_from_center[k,:].T

        logR[:, c] = ( np.log(pi[c])
            + 0.5 * log_det_term
            - 0.5*d*np.log(sigma2[c])
            - 0.5 * quad_term
            )

    # log-sum-expトリックによる正規化
    myMax = np.zeros(N)
    for k in range(N):
        myMax[k] = np.max(logR[k, :])

    log_sum_exp = np.zeros(N)
    for k in range(N):
        log_sum_exp[k] = myMax[k] + np.log(np.sum(np.exp(logR[k, :] - myMax[k])))

    for k in range(N):
        logR[k, :] = logR[k, :] - log_sum_exp[k]
        
    R = np.exp(logR)

    return R
```

---

./prmsl_era5_all_data_seasonal_small.ncのラベルの説明
基本の型は15種類あります
- 1
- 2A
- 2B
- 2C
- 2D
- 3A
- 3B
- 3C
- 3D
- 4A
- 4B
- 5
- 6A
- 6B
- 6C

これ以外のラベルとして複合型という物があります
上記の15種類のどれかから2つ選ばれて、-もしくは+が真ん中についた物です（例：1+2A、4A-6B）

---

以下のファイルやデータをインポートして自分の研究を行うプログラム（main_v1.py）を作成してください
./mppca.py
./prmsl_era5_all_data_seasonal_small.nc
./minisom.py

MPPCAのパラメータは、
p : クラスタの数は100、q : 潜在空間の次元数は2、niter : EMアルゴリズムの反復回数は100000にしてください

MiniSomのパラメータは、
input_lenはMPPCAの結果、sigma=3.0、learning_rate=1.0、neighborhood_function='gaussian'、topology='rectangular'サイズは10×10、activation_distance='euclidean'、num_iterationsは100000、random_seedはプログラム全体で再現性を高めるためにGLOBAL_SEEDを全体に導入してください

Minisomでの分類後、各ノードに分類されたデータのラベルを確認して欲しいです
評価指標はマクロ平均再現率を使用してください
複合ラベルがノード内に確認された場合は2つともカウントするようにしてください（1のラベルの場合はそのノードでは1をカウントしますが、例えば4A+2Cのラベルのデータがあった場合は4Aにも2Cカウントするようにしてください）

上記の結果を視覚的に見やすいように、図表の出力をさまざまな形で行なってください
japanize_matplotlibを使用してください

./prmsl_era5_all_data_seasonal_small.ncから持ってくるデータ範囲は
全データ
    1958年1月1日～2023年12月31日：24106個
ラベル付きデータ
    1991年1月1日〜2000年12月31日：3653個
のようになっているのですが、ラベル付きのデータのみをあつかうようにしてください

プログラムについて処理を行う際は随時どのくらいかかるかや実行時間などのログを細かくprintで表示させるようにしてください

基本的には一番上の研究計画に沿った実装にしてください
完璧なmain_v1.pyを作成してプログラムを全文省略などせずに表示してください
プログラム作成後は処理や内容について詳しく日本語で解説