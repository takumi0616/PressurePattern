/home/takumi/docker_miniconda/src/PressurePattern/SOM-MPPCA/main_v2_pytorch.py
```
# main_v2_pytorch.py
# -*- coding: utf-8 -*-
"""
main_v2_pytorch.py (PyTorch GPU対応版)
  
高次元データ構造と不確実性の可視化：ハイブリッドMPPCA-SOMフレームワーク
海面気圧データ（ERA5）を用いた気圧配置パターンの解析
  
[主な改善点]
- MPPCAの計算をPyTorchで実行し、幅広いGPU環境での高速化を実現。
- Python標準の`logging`モジュールを導入し、コンソールとファイルにログを記録。
- 時間のかかるMPPCAとSOMの訓練ループに`tqdm`プログレスバーを適用し、進捗を可視化。
- GPUが利用可能か事前にチェックする機能を追加。
- すべての可視化機能を実装し、一貫したロギングを行う。
"""
import os
import time
import pickle
import warnings
import logging
from collections import Counter, defaultdict

# --- PyTorch ライブラリのインポート ---
import torch

# --- 他ライブラリのインポート ---
import numpy as np
import xarray as xr
import matplotlib.pyplot as plt
import japanize_matplotlib
from scipy.stats import entropy
from sklearn.metrics import recall_score
from sklearn.preprocessing import MultiLabelBinarizer
from tqdm import tqdm

from minisom import MiniSom
# 新しく作成したPyTorch版MPPCAをインポート
from mppca_pytorch import initialization_kmeans_torch, mppca_gem_torch

# --- 1. グローバル設定とパラメータ定義 ---

# 再現性のためのシード
GLOBAL_SEED = 42
np.random.seed(GLOBAL_SEED)
torch.manual_seed(GLOBAL_SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(GLOBAL_SEED)

# 出力ディレクトリ設定
OUTPUT_DIR = "mppca_som_results_pytorch"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ログ設定
LOG_FILE = os.path.join(OUTPUT_DIR, 'execution_pytorch.log')
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(LOG_FILE, mode='w', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# GPU利用可能チェックとデバイス設定
GPU_ENABLED = torch.cuda.is_available()
DEVICE = torch.device("cuda" if GPU_ENABLED else "cpu")
if GPU_ENABLED:
    logging.info(f"GPUが利用可能です。デバイス: {torch.cuda.get_device_name(0)}")
else:
    logging.info("GPUが利用できません。CPUで実行します。")


# データファイルパス
DATA_FILE_PATH = './prmsl_era5_all_data_seasonal_small.nc'

# モデルパラメータ
# MPPCA
P_CLUSTERS = 49 # 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361
Q_LATENT_DIM = 2
N_ITER_MPPCA = 500 # テスト時は短く、本番では長く設定 (例: 100)
MPPCA_BATCH_SIZE = 1024


# MiniSom
MAP_X, MAP_Y = 7, 7
SIGMA_SOM = 3.0
LEARNING_RATE_SOM = 1.0
NEIGHBORHOOD_FUNCTION_SOM = 'gaussian'
TOPOLOGY_SOM = 'rectangular'
ACTIVATION_DISTANCE_SOM = 'euclidean'
N_ITER_SOM = 10000000 # SOMの学習イテレーション

# 基本となる気圧配置パターンのラベル
BASE_LABELS = [
    '1', '2A', '2B', '2C', '2D', '3A', '3B', '3C', '3D',
    '4A', '4B', '5', '6A', '6B', '6C'
]

# --- 2. 補助関数 (変更なし) ---
def format_duration(seconds):
    m, s = divmod(seconds, 60)
    h, m = divmod(m, 60)
    return f"{int(h):02d}:{int(m):02d}:{int(s):02d}"

def parse_label(label_str):
    return label_str.replace('+', '-').split('-')

# --- 3. データ処理関数 (変更なし) ---
def load_and_preprocess_data(filepath, start_date='1991-01-01', end_date='2000-12-31'):
    logging.info(f"データファイル '{filepath}' を読み込み中...")
    start_time = time.time()
    try:
        with xr.open_dataset(filepath, engine='netcdf4') as ds:
            ds_labeled = ds.sel(valid_time=slice(start_date, end_date))
            # ★★★ データ読み込み時点でfloat64に統一 ★★★
            msl_data = ds_labeled['msl'].values.astype(np.float64)
            labels = ds_labeled['label'].values.astype(str)
            lat = ds_labeled['latitude'].values
            lon = ds_labeled['longitude'].values
    except FileNotFoundError:
        logging.error(f"データファイルが見つかりません: {filepath}")
        raise
    logging.info(f"データ読み込み完了。対象期間: {start_date} ～ {end_date}")
    n_samples, n_lat, n_lon = msl_data.shape
    logging.info(f"サンプル数: {n_samples}, 緯度点数: {n_lat}, 経度点数: {n_lon}")
    data_reshaped = msl_data.reshape(n_samples, n_lat * n_lon)
    if np.isnan(data_reshaped).any():
        logging.info("欠損値を検出。列ごとの平均値で補完します...")
        col_mean = np.nanmean(data_reshaped, axis=0)
        inds = np.where(np.isnan(data_reshaped))
        data_reshaped[inds] = np.take(col_mean, inds[1])
    logging.info("データを標準化（Z-score normalization）します...")
    data_mean = data_reshaped.mean(axis=0)
    data_std = data_reshaped.std(axis=0)
    data_std[data_std == 0] = 1
    data_normalized = (data_reshaped - data_mean) / data_std
    end_time = time.time()
    logging.info(f"データ前処理完了。所要時間: {format_duration(end_time - start_time)}")
    # 全てfloat64で返すようにする
    return data_normalized.astype(np.float64), labels, lat, lon, data_mean.astype(np.float64), data_std.astype(np.float64)

# --- 4. モデル実行関数 ---

def run_mppca_pytorch(data_np, p, q, niter):
    """MPPCAモデルをPyTorchで訓練し、結果をCPUのNumPy配列で返す"""
    logging.info("MPPCAの訓練を開始します (PyTorch版)...")
    logging.info(f"パラメータ: クラスター数(p)={p}, 潜在次元(q)={q}, 反復回数(niter)={niter}")
    start_time = time.time()
    data_t = torch.from_numpy(data_np.astype(np.float64))
    logging.info(f"K-meansによる初期化を実行中 ({DEVICE})...")
    
    pi_t, mu_t, W_t, sigma2_t, _ = initialization_kmeans_torch(
        data_t, p, q, device=DEVICE
    )
    logging.info("K-means初期化完了。")

    logging.info(f"GEMアルゴリズムによる訓練を実行中 ({DEVICE})...")
    pi_t, mu_t, W_t, sigma2_t, R_t, L_t, _ = mppca_gem_torch(
        X=data_t,
        pi=pi_t, 
        mu=mu_t, 
        W=W_t, 
        sigma2=sigma2_t,
        niter=niter,
        batch_size=MPPCA_BATCH_SIZE,
        device=DEVICE
    )

    end_time = time.time()
    logging.info(f"MPPCA訓練完了。所要時間: {format_duration(end_time - start_time)}")
    
    logging.info("結果をGPUからCPUに転送し、NumPy配列に変換中...")
    L_cpu = L_t.cpu().numpy()
    if L_cpu is not None and len(L_cpu) > 0:
        final_log_likelihood = L_cpu[-1]
        if np.isnan(final_log_likelihood):
            logging.warning("最終的な対数尤度が'nan'です。モデルが正しく収束しなかった可能性があります。")
        else:
            logging.info(f"最終的な対数尤度: {final_log_likelihood:.4f}")

    mppca_results = {
        'pi': pi_t.cpu().numpy(), 'mu': mu_t.cpu().numpy(), 'W': W_t.cpu().numpy(),
        'sigma2': sigma2_t.cpu().numpy(), 'R': R_t.cpu().numpy(), 'L': L_cpu
    }
    
    result_path = os.path.join(OUTPUT_DIR, 'mppca_results_pytorch.pkl')
    with open(result_path, 'wb') as f:
        pickle.dump(mppca_results, f)
    logging.info(f"MPPCAのモデルと結果を '{result_path}' に保存しました。")
    
    return R_t.cpu().numpy(), L_cpu

def run_som(data, map_x, map_y, input_len, sigma, lr, n_iter, seed):
    """MiniSomを訓練し、訓練済みモデルを返す (変更なし)"""
    logging.info("SOMの訓練を開始します...")
    logging.info(f"パラメータ: マップサイズ={map_x}x{map_y}, sigma={sigma}, learning_rate={lr}, 反復回数={n_iter}")
    start_time = time.time()
    
    som = MiniSom(map_x, map_y, input_len,
                  sigma=sigma, learning_rate=lr,
                  neighborhood_function=NEIGHBORHOOD_FUNCTION_SOM,
                  topology=TOPOLOGY_SOM, activation_distance=ACTIVATION_DISTANCE_SOM,
                  random_seed=seed)
    
    if np.isnan(data).any():
        logging.warning("SOMへの入力データにnanが含まれています。SOMの重み初期化と学習に影響する可能性があります。")
        som.random_weights_init(np.nan_to_num(data))
    else:
        som.random_weights_init(data)

    logging.info("SOM訓練中 (進捗表示あり)...")
    q_error_history = som.train_batch(data, n_iter, verbose=True)
    
    end_time = time.time()
    logging.info(f"SOM訓練完了。所要時間: {format_duration(end_time - start_time)}")
    
    model_path = os.path.join(OUTPUT_DIR, 'som_model_pytorch.pkl')
    with open(model_path, 'wb') as f:
        pickle.dump(som, f)
    logging.info(f"SOMモデルを '{model_path}' に保存しました。")
    
    return som, q_error_history

# --- 5. 評価・分析関数 (変更なし) ---
def evaluate_classification(som, mppca_posterior, original_labels):
    logging.info("分類性能の評価を開始します (マクロ平均再現率)...")
    start_time = time.time()
    
    if np.isnan(mppca_posterior).any():
        logging.error("MPPCAの事後確率にnanが含まれているため、評価をスキップします。")
        return 0.0, {}

    parsed_true_labels = [parse_label(l) for l in original_labels]
    mlb = MultiLabelBinarizer(classes=BASE_LABELS)
    y_true = mlb.fit_transform(parsed_true_labels)
    win_map_indices = som.win_map(mppca_posterior, return_indices=True)
    node_dominant_label = {}
    for pos, indices in win_map_indices.items():
        if not indices:
            node_dominant_label[pos] = None
            continue
        labels_in_node = [label for i in indices for label in parsed_true_labels[i]]
        if not labels_in_node:
            node_dominant_label[pos] = None
            continue
        most_common = Counter(labels_in_node).most_common(1)
        node_dominant_label[pos] = most_common[0][0]
    winners = [som.winner(x) for x in mppca_posterior]
    predicted_single_labels = [node_dominant_label.get(w) for w in winners]
    y_pred = mlb.transform([[l] if l else [] for l in predicted_single_labels])
    macro_recall = recall_score(y_true, y_pred, average='macro', zero_division=0)
    logging.info(f"評価完了。所要時間: {format_duration(time.time() - start_time)}")
    logging.info(f"マクロ平均再現率: {macro_recall:.4f}")
    per_class_recall = recall_score(y_true, y_pred, average=None, zero_division=0)
    recall_scores_str = "\n".join([f" - {label:<4}: {recall:.4f}" for label, recall in zip(mlb.classes_, per_class_recall)])
    logging.info(f"クラスごとの再現率:\n{recall_scores_str}")
    return macro_recall, node_dominant_label

# --- 6. 可視化関数 ---
def plot_log_likelihood(log_likelihood_history):
    """MPPCAの対数尤度の収束グラフをプロットして保存する"""
    logging.info("対数尤度の収束グラフを作成中...")
    plt.figure(figsize=(10, 6))
    
    # 履歴にnanが含まれている場合、それらを除外してプロット
    valid_indices = ~np.isnan(log_likelihood_history)
    if not np.any(valid_indices):
        logging.warning("有効な対数尤度データがないため、収束グラフは作成されません。")
        plt.close()
        return
        
    iterations = np.arange(len(log_likelihood_history))[valid_indices]
    valid_likelihoods = log_likelihood_history[valid_indices]

    plt.plot(iterations, valid_likelihoods)
    plt.title('MPPCA 対数尤度の収束履歴')
    plt.xlabel('イテレーション (Iteration)')
    plt.ylabel('対数尤度 (Log-Likelihood)')
    plt.grid(True)
    plt.tight_layout()
    save_path = os.path.join(OUTPUT_DIR, 'log_likelihood_convergence_pytorch.png')
    plt.savefig(save_path, dpi=300)
    plt.close()
    logging.info(f"対数尤度グラフを '{save_path}' に保存しました。")

def plot_quantization_error(q_error_history, log_interval):
    """SOMの量子化誤差の収束グラフをプロットして保存する"""
    logging.info("SOM量子化誤差の収束グラフを作成中...")
    plt.figure(figsize=(10, 6))
    
    iterations = np.arange(len(q_error_history)) * log_interval
    plt.plot(iterations, q_error_history)
    plt.title('SOM 量子化誤差の収束履歴')
    plt.xlabel('イテレーション (Iteration)')
    plt.ylabel('量子化誤差 (Quantization Error)')
    plt.grid(True)
    plt.tight_layout()
    save_path = os.path.join(OUTPUT_DIR, 'som_quantization_error_pytorch.png')
    plt.savefig(save_path, dpi=300)
    plt.close()
    logging.info(f"SOM量子化誤差グラフを '{save_path}' に保存しました。")

def visualize_results(som, mppca_posterior, original_data, labels, node_dominant_label, lat, lon, data_mean, data_std, log_likelihood_history, q_error_history):
    logging.info("結果の可視化を開始します...")
    start_time = time.time()

    plot_log_likelihood(log_likelihood_history)
    plot_quantization_error(q_error_history, log_interval=1000) # minisom.pyのlog_intervalと値を合わせる
    
    if np.isnan(mppca_posterior).any():
        logging.error("MPPCAの事後確率にnanが含まれているため、可視化をスキップします。")
        return
        
    map_x, map_y = som.get_weights().shape[:2]
    
    plt.figure(figsize=(10, 10)); plt.pcolor(som.distance_map().T, cmap='bone_r'); plt.colorbar(label='ニューロン間の距離'); plt.title('U-Matrix'); plt.grid(True); plt.savefig(os.path.join(OUTPUT_DIR, 'u_matrix_pytorch.png'), dpi=300); plt.close()
    logging.info("U-Matrixを保存しました。")
    
    plt.figure(figsize=(10, 10)); frequencies = som.activation_response(mppca_posterior); plt.pcolor(frequencies.T, cmap='viridis'); plt.colorbar(label='勝者となった回数'); plt.title('ヒットマップ'); plt.grid(True); plt.savefig(os.path.join(OUTPUT_DIR, 'hit_map_pytorch.png'), dpi=300); plt.close()
    logging.info("ヒットマップを保存しました。")
    
    plt.figure(figsize=(12, 10)); label_map = np.full((map_x, map_y), -1, dtype=int); label_names = {label: i for i, label in enumerate(BASE_LABELS)};
    for pos, label in node_dominant_label.items():
        if label: label_map[pos[0], pos[1]] = label_names[label]
    cmap = plt.get_cmap('tab20', len(BASE_LABELS)); plt.pcolor(label_map.T, cmap=cmap, vmin=-0.5, vmax=len(BASE_LABELS)-0.5); cbar = plt.colorbar(ticks=np.arange(len(BASE_LABELS))); cbar.ax.set_yticklabels(BASE_LABELS); plt.title('勝者総取りマップ'); plt.grid(True); plt.savefig(os.path.join(OUTPUT_DIR, 'winner_takes_all_map_pytorch.png'), dpi=300); plt.close()
    logging.info("勝者総取りマップを保存しました。")

    win_map_indices = som.win_map(mppca_posterior, return_indices=True)
    entropy_map = np.full((map_x, map_y), np.nan)
    for pos, indices in win_map_indices.items():
        if indices: entropy_map[pos] = entropy(mppca_posterior[indices].mean(axis=0))
    plt.figure(figsize=(10, 10)); plt.pcolor(entropy_map.T, cmap='magma'); plt.colorbar(label='エントロピー'); plt.title('エントロピーマップ'); plt.grid(True); plt.savefig(os.path.join(OUTPUT_DIR, 'entropy_map_pytorch.png'), dpi=300); plt.close()
    logging.info("エントロピーマップを保存しました。")
    
    valid_nodes = {pos: indices for pos, indices in win_map_indices.items() if indices}
    num_representative_nodes = min(9, len(valid_nodes))
    if num_representative_nodes > 0:
        logging.info("代表的なノードの平均気圧配置図を作成中...")
        sorted_nodes = sorted(valid_nodes.keys(), key=lambda pos: frequencies[pos], reverse=True)
        top_nodes = sorted_nodes[:num_representative_nodes]
        fig, axes = plt.subplots(3, 3, figsize=(15, 15), constrained_layout=True)
        fig.suptitle('ヒット数の多い代表的ノードの平均気圧配置', fontsize=20)
        axes = axes.flatten()
        for i, pos in enumerate(top_nodes):
            ax = axes[i]
            indices = win_map_indices[pos]
            node_data_original_scale = original_data[indices] * data_std + data_mean
            mean_pattern = node_data_original_scale.mean(axis=0).reshape(len(lat), len(lon))
            ax.contourf(lon, lat, mean_pattern / 100, cmap='coolwarm', levels=20)
            ax.contour(lon, lat, mean_pattern / 100, colors='k', linewidths=0.5, levels=15)
            ax.set_title(f"Node {pos} (n={len(indices)})\nDom: {node_dominant_label.get(pos, 'N/A')}")
        for i in range(num_representative_nodes, len(axes)): axes[i].set_visible(False)
        plt.savefig(os.path.join(OUTPUT_DIR, 'representative_node_patterns_pytorch.png'), dpi=300)
        plt.close()
        logging.info("代表的なノードの平均気圧配置図を保存しました。")
    else:
        logging.warning("代表的なノードが見つからず、気圧配置図は作成しませんでした。")
    
    end_time = time.time()
    logging.info(f"可視化完了。所要時間: {format_duration(end_time - start_time)}")


# --- 7. メイン実行ブロック --- 
def main(): 
    """メインの処理フロー""" 
    main_start_time = time.time() 
    logging.info("======= 研究プログラムを開始します (PyTorch版) =======") 

    # 1: データ読み込みと前処理 
    data_normalized, labels, lat, lon, data_mean, data_std = load_and_preprocess_data(DATA_FILE_PATH) 
    
    # 2: MPPCAの実行 (PyTorch) 
    mppca_posterior, log_likelihood_history = run_mppca_pytorch(data_normalized, P_CLUSTERS, Q_LATENT_DIM, N_ITER_MPPCA) 

    # 3: SOMの実行 (CPU, NumPy) 
    som, q_error_history = run_som(mppca_posterior, MAP_X, MAP_Y, P_CLUSTERS, SIGMA_SOM, LEARNING_RATE_SOM, N_ITER_SOM, GLOBAL_SEED) 
    
    # 4: 評価 
    _, node_dominant_label = evaluate_classification(som, mppca_posterior, labels) 
    
    # 5: 可視化 
    # 元データを渡す必要があるので、逆標準化は可視化関数内で行う 
    visualize_results(som, mppca_posterior, data_normalized, labels, node_dominant_label, lat, lon, data_mean, data_std, log_likelihood_history, q_error_history) 
    
    main_end_time = time.time() 
    logging.info("======= すべての処理が完了しました =======") 
    logging.info(f"総実行時間: {format_duration(main_end_time - main_start_time)}")


if __name__ == '__main__':
    warnings.filterwarnings('ignore', category=UserWarning, module='minisom')
    main()
```


/home/takumi/docker_miniconda/src/PressurePattern/SOM-MPPCA/main.py
```
import os
import logging
import torch
import xarray as xr
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
import cartopy.crs as ccrs
import cartopy.feature as cfeature
from matplotlib.colors import Normalize
import seaborn as sns
import random 
from mppca_pytorch import initialization_kmeans_torch, mppca_gem_torch
GLOBAL_SEED = 42

def set_global_seed(seed):
    """
    全てのライブラリの乱数シードを固定し、再現性を高める。
    """
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    logging.info(f"グローバルシード {seed} を設定し、決定論的動作を有効にしました。")

# --- 設定項目 ---
# ファイルパスとディレクトリ
DATA_FILE = './prmsl_era5_all_data_seasonal_small.nc' 
RESULT_DIR = './mppca_results'

# データ期間
START_DATE = '1991-01-01'
END_DATE = '2000-12-31'

# MPPCAモデルのパラメータ
P_CLUSTERS = 4
Q_LATENT_DIM = 20
N_ITERATIONS = 500
BATCH_SIZE = 1024

# 基本ラベルリスト
BASE_LABELS = [
    '1', '2A', '2B', '2C', '2D', '3A', '3B', '3C', '3D',
    '4A', '4B', '5', '6A', '6B', '6C'
]

# --- ロギングと結果ディレクトリの設定 ---
os.makedirs(RESULT_DIR, exist_ok=True)
log_path = os.path.join(RESULT_DIR, 'verification.log')
if os.path.exists(log_path): os.remove(log_path)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[logging.FileHandler(log_path), logging.StreamHandler()])

# --- データ読み込みと前処理 ---
def load_and_prepare_data(filepath, start_date, end_date):
    """データとラベル、座標を読み込み、前処理を行う"""
    logging.info(f"データファイル '{filepath}' を読み込んでいます...")
    try:
        ds = xr.open_dataset(filepath)
    except FileNotFoundError:
        logging.error(f"データファイルが見つかりません: {filepath}"); return None, None, None, None, None
        
    data_period = ds.sel(valid_time=slice(start_date, end_date))
    
    if data_period.valid_time.size == 0:
        logging.error("指定された期間にデータが見つかりませんでした。"); return None, None, None, None, None
    
    if 'label' in data_period.variables:
        labels = [l.decode('utf-8') if isinstance(l, bytes) else str(l) for l in data_period['label'].values]
        logging.info("ラベルデータを読み込みました。")
    else:
        logging.warning("'label'変数がデータファイルに見つかりませんでした。"); labels = None

    msl_data = data_period['msl']
    if msl_data.isnull().any():
        logging.warning("データに欠損値 (NaN) が見つかりました。データ全体の平均値で補完します。")
        msl_data = msl_data.fillna(msl_data.mean().item())
        
    n_samples, d_lat, d_lon = msl_data.shape[0], msl_data.shape[1], msl_data.shape[2]
    logging.info(f"データ形状: {n_samples}サンプル, {d_lat*d_lon}次元 ({d_lat}x{d_lon})")
    flattened_data = msl_data.values.reshape(n_samples, d_lat * d_lon)
    lat_coords = data_period['latitude'].values
    lon_coords = data_period['longitude'].values
    return torch.from_numpy(flattened_data), lat_coords, lon_coords, data_period.valid_time.values, labels

# --- 計算・可視化・分析関数 ---

def format_coord_vector(vector, precision=4):
    """潜在空間の座標ベクトルを整形して複数行の文字列として返す"""
    # 10要素ごとに改行を入れる
    return np.array2string(
        vector,
        formatter={'float_kind': lambda x: f"{x:.{precision}f}"},
        separator=', ',
        max_line_width=100  # 1行あたりの最大文字数
    )

def analyze_kmeans_clusters(clusters, labels, time_stamps, p):
    """k-meansクラスタリングの結果を分析し、ラベルや月別分布などを表示する"""
    logging.info("\n--- k-meansクラスタリング結果の総合分析 ---")
    clusters_np = clusters.cpu().numpy()
    for i in range(p):
        indices = np.where(clusters_np == i)[0]; num_samples_in_cluster = len(indices)
        logging.info(f"\n[k-meansクラスタ {i+1}]"); logging.info(f"  - データポイント数: {num_samples_in_cluster}")
        if num_samples_in_cluster == 0: continue
        if labels is not None:
            label_counts = {label: 0 for label in BASE_LABELS}; total_label_counts = 0
            cluster_labels = [labels[j] for j in indices]
            for label_str in cluster_labels:
                sub_labels = label_str.split('+') if '+' in label_str else (label_str.split('-') if '-' in label_str else [label_str])
                for sub_label in sub_labels:
                    sub_label = sub_label.strip()
                    if sub_label in label_counts: label_counts[sub_label] += 1; total_label_counts += 1
            if total_label_counts > 0:
                logging.info("  - ラベル構成 (カウント数順):")
                sorted_labels = sorted(label_counts.items(), key=lambda item: item[1], reverse=True)
                for label, count in sorted_labels:
                    if count > 0: logging.info(f"    - {label:<4}: {count:4d} 件 ({(count / total_label_counts) * 100:5.1f}%)")
        cluster_time_stamps = time_stamps[indices]; months = pd.to_datetime(cluster_time_stamps).month
        month_counts = {m: 0 for m in range(1, 13)}; unique_months, counts = np.unique(months, return_counts=True)
        for month_val, count in zip(unique_months, counts): month_counts[month_val] = count
        logging.info("  - 月別分布:")
        for month_val in range(1, 13):
            count = month_counts[month_val]; percentage = (count / num_samples_in_cluster) * 100 if num_samples_in_cluster > 0 else 0
            bar = '■' * int(percentage / 4); logging.info(f"    - {month_val:2d}月: {count:4d}件 ({percentage:5.1f}%) {bar}")
    logging.info("\n--- 総合分析終了 ---")

def calculate_latent_coords(X, R, mu, W, sigma2, device):
    """各データポイントが所属する主クラスタにおける潜在空間座標を計算する"""
    p, d, q = W.shape; N, _ = X.shape; cluster_assignments = torch.argmax(R, dim=1)
    latent_coords = torch.zeros(N, q, device=device, dtype=X.dtype)
    W_T = W.transpose(-2, -1); I_q = torch.eye(q, device=device, dtype=X.dtype)
    logging.info("主潜在空間の座標を計算中...")
    for i in tqdm(range(p), desc="Calculating main latent coords"):
        mask = (cluster_assignments == i)
        if mask.sum() == 0: continue
        X_c, mu_c, W_c, W_T_c, sigma2_c = X[mask], mu[i], W[i], W_T[i], sigma2[i]
        M_c = sigma2_c * I_q + W_T_c @ W_c; M_inv_c = torch.linalg.inv(M_c)
        deviation = X_c - mu_c; coords = (M_inv_c @ W_T_c @ deviation.T).T
        latent_coords[mask] = coords
    return latent_coords, cluster_assignments

def calculate_all_latent_coords_for_sample(X_sample, mu, W, sigma2, device):
    """単一のデータポイントについて、全てのクラスタにおける潜在空間座標を計算する"""
    p, d, q = W.shape; all_coords = torch.zeros(p, q, device=device, dtype=X_sample.dtype)
    W_T = W.transpose(-2, -1); I_q = torch.eye(q, device=device, dtype=X_sample.dtype)
    for i in range(p):
        mu_c, W_c, W_T_c, sigma2_c = mu[i], W[i], W_T[i], sigma2[i]
        M_c = sigma2_c * I_q + W_T_c @ W_c; M_inv_c = torch.linalg.inv(M_c)
        deviation = X_sample - mu_c; coords = M_inv_c @ W_T_c @ deviation
        all_coords[i] = coords
    return all_coords

def plot_log_likelihood(log_L, save_path):
    """対数尤度の収束グラフをプロットして保存する"""
    log_L_np = log_L.cpu().numpy()
    # NaNやinfをプロットから除外する
    finite_vals = np.isfinite(log_L_np)
    if not np.any(finite_vals):
        logging.warning("対数尤度の値がすべて無効なため、グラフを生成できません。")
        return
        
    plt.figure(figsize=(10, 6))
    plt.plot(np.arange(len(log_L_np))[finite_vals], log_L_np[finite_vals])
    plt.title('Log-Likelihood Convergence')
    plt.xlabel('Iteration'); plt.ylabel('Log-Likelihood'); plt.grid(True); plt.savefig(save_path); plt.close()
    logging.info(f"対数尤度グラフを保存: {save_path}")

def plot_average_patterns(mu, lat_coords, lon_coords, data_mean, save_path):
    """学習された平均的な気圧配置パターンを、地図上に偏差としてプロットする"""
    p, _ = mu.shape
    n_cols = 5
    n_rows = (p + n_cols - 1) // n_cols if p > 0 else 1

    cmap = sns.color_palette("RdBu_r", as_cmap=True)
    pressure_vmin, pressure_vmax = -12, 12
    pressure_levels = np.linspace(pressure_vmin, pressure_vmax, 25)
    pressure_norm = Normalize(vmin=pressure_vmin, vmax=pressure_vmax)
    
    fig, axes = plt.subplots(
        n_rows, n_cols, figsize=(n_cols * 3.5, n_rows * 3),
        subplot_kw={"projection": ccrs.PlateCarree()}
    )
    axes = np.atleast_1d(axes).flatten()
    fig.subplots_adjust(wspace=0.05, hspace=0.05)
    
    mu_np = mu.cpu().numpy()
    data_mean_np = data_mean.cpu().numpy()

    for i in range(p):
        ax = axes[i]
        mean_pattern_hpa = (mu_np[i] / 100.0) - (data_mean_np / 100.0)
        mean_pattern_2d = mean_pattern_hpa.reshape(len(lat_coords), len(lon_coords))
        
        cont = ax.contourf(lon_coords, lat_coords, mean_pattern_2d, levels=pressure_levels, cmap=cmap, extend="both", norm=pressure_norm)
        ax.contour(lon_coords, lat_coords, mean_pattern_2d, levels=pressure_levels, colors="k", linewidths=0.5)
        ax.add_feature(cfeature.COASTLINE.with_scale("50m"), edgecolor="black", linewidth=0.5)
        ax.set_extent([120, 150, 20, 50], crs=ccrs.PlateCarree())
        ax.set_title(f'Cluster {i+1}', loc='left', fontsize=10)

    for i in range(p, len(axes)):
        axes[i].axis("off")
        
    plt.suptitle('Learned Average Pressure Anomaly Patterns (μ - mean) [hPa]', fontsize=16)
    fig.tight_layout(rect=[0, 0, 0.9, 1])
    
    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])
    fig.colorbar(cont, cax=cbar_ax, label="Pressure Anomaly (hPa)", ticks=np.arange(pressure_vmin, pressure_vmax + 1, 4))
    
    plt.savefig(save_path)
    plt.close()
    logging.info(f"平均パターン画像を保存: {save_path}")

def plot_latent_space(latent_coords, cluster_assignments, p, save_path):
    """潜在空間の分布を2次元で可視化して保存する（最初の2次元のみ使用）"""
    latent_coords_np = latent_coords.cpu().numpy(); cluster_assignments_np = cluster_assignments.cpu().numpy()
    plt.figure(figsize=(12, 10)); scatter = plt.scatter(latent_coords_np[:, 0], latent_coords_np[:, 1], c=cluster_assignments_np, cmap='tab20', alpha=0.6, s=10)
    plt.title('Latent Space Visualization (First 2 Dimensions)'); plt.xlabel('Latent Dimension 1'); plt.ylabel('Latent Dimension 2')
    plt.legend(handles=scatter.legend_elements()[0], labels=[f'Cluster {i+1}' for i in range(p)], bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True); plt.tight_layout(rect=[0, 0, 0.85, 1]); plt.savefig(save_path); plt.close()
    logging.info(f"潜在空間プロットを保存: {save_path}")

def plot_reconstructions(X, mu, W, latent_coords, cluster_assignments, lat_coords, lon_coords, data_mean, time_stamps, n_samples, save_path):
    """元のデータ、平均パターン、再構成データを地図上に偏差として比較プロットする"""
    fig, axes = plt.subplots(
        n_samples, 3, figsize=(10, 3 * n_samples),
        subplot_kw={"projection": ccrs.PlateCarree()}
    )
    fig.suptitle('Original vs. Average Pattern vs. Reconstructed Anomaly [hPa]', fontsize=16)
    
    cmap = sns.color_palette("RdBu_r", as_cmap=True)
    pressure_vmin, pressure_vmax = -20, 20
    pressure_levels = np.linspace(pressure_vmin, pressure_vmax, 21)
    pressure_norm = Normalize(vmin=pressure_vmin, vmax=pressure_vmax)

    X_np, mu_np = X.cpu().numpy(), mu.cpu().numpy()
    data_mean_np = data_mean.cpu().numpy()

    for i in range(n_samples):
        original_vec = X_np[i]; assigned_cluster = cluster_assignments[i].item(); latent_vec = latent_coords[i]
        W_c, mu_c = W[assigned_cluster], mu[assigned_cluster]
        reconstructed_vec = (W_c @ latent_vec + mu_c).cpu().numpy()
        avg_pattern_vec = mu_np[assigned_cluster]
        
        patterns_to_plot = {
            "Original": original_vec,
            "Avg Pattern": avg_pattern_vec,
            "Reconstructed": reconstructed_vec
        }
        
        for j, (title, pattern_vec) in enumerate(patterns_to_plot.items()):
            ax = axes[i, j]
            anomaly_hpa = (pattern_vec / 100.0) - (data_mean_np / 100.0)
            anomaly_2d = anomaly_hpa.reshape(len(lat_coords), len(lon_coords))
            
            ax.contourf(lon_coords, lat_coords, anomaly_2d, levels=pressure_levels, cmap=cmap, extend="both", norm=pressure_norm)
            ax.contour(lon_coords, lat_coords, anomaly_2d, levels=pressure_levels, colors="k", linewidths=0.5)
            ax.add_feature(cfeature.COASTLINE.with_scale("50m"), edgecolor="black", linewidth=0.5)
            ax.set_extent([120, 150, 20, 50], crs=ccrs.PlateCarree())
            
            if j == 0:
                date_str = pd.to_datetime(str(time_stamps[i])).strftime('%Y-%m-%d')
                ax.set_title(f'{title}\n({date_str})', fontsize=10)
            elif j == 1:
                ax.set_title(f'{title}\n(Cluster {assigned_cluster+1})', fontsize=10)
            else:
                ax.set_title(title, fontsize=10)

    fig.tight_layout(rect=[0, 0, 1, 0.96]); plt.savefig(save_path); plt.close()
    logging.info(f"再構成画像の比較プロットを保存: {save_path}")

# --- メイン処理 ---
if __name__ == '__main__':
    logging.info("--- MPPCA検証プログラム開始 ---")
    # プログラム開始時にグローバルシードを設定
    set_global_seed(GLOBAL_SEED)
    logging.info("=================================================="); logging.info("               実験条件サマリー"); logging.info("==================================================")
    logging.info(f"入力データファイル: {DATA_FILE}"); logging.info(f"結果出力ディレクトリ: {RESULT_DIR}"); logging.info(f"ログファイル: {log_path}")
    logging.info("--------------------------------------------------"); logging.info(f"データ対象期間: {START_DATE} から {END_DATE}")
    logging.info("--------------------------------------------------"); logging.info("MPPCAモデル パラメータ:")
    logging.info(f"  - クラスター数 (P_CLUSTERS): {P_CLUSTERS}"); logging.info(f"  - 潜在変数の次元 (Q_LATENT_DIM): {Q_LATENT_DIM}")
    logging.info(f"  - EMアルゴリズム反復回数 (N_ITERATIONS): {N_ITERATIONS}"); logging.info(f"  - バッチサイズ (BATCH_SIZE): {BATCH_SIZE}")
    logging.info("--------------------------------------------------"); device = 'cuda' if torch.cuda.is_available() else 'cpu'; logging.info(f"使用デバイス: {device.upper()}")
    logging.info("==================================================\n")

    X_original_dtype, lat_coords, lon_coords, time_stamps, labels = load_and_prepare_data(DATA_FILE, START_DATE, END_DATE)

    if X_original_dtype is not None:
        # ---【重要】数値安定性のために倍精度(float64)を使用 ---
        # 高次元データや反復計算では、単精度(float32)では計算誤差が蓄積し、
        # 発散の原因となるため、倍精度に変換して計算を行います。
        X = X_original_dtype.to(device, dtype=torch.float64)
        data_mean = X.mean(0) # 全データの平均を計算
        
        logging.info("k-meansによる初期化を開始..."); 
        pi, mu, W, sigma2, kmeans_clusters = initialization_kmeans_torch(X, P_CLUSTERS, Q_LATENT_DIM, device=device)
        if labels is not None: analyze_kmeans_clusters(kmeans_clusters, labels, time_stamps, P_CLUSTERS)
        logging.info(f"k-means初期化完了。pi:{pi.shape}, mu:{mu.shape}, W:{W.shape}, sigma2:{sigma2.shape}")
        
        logging.info("MPPCAモデルの学習 (GEMアルゴリズム) を開始..."); 
        # パラメータも入力データXと同じデバイスとデータ型に統一して渡す
        pi, mu, W, sigma2 = pi.to(X.device, X.dtype), mu.to(X.device, X.dtype), W.to(X.device, X.dtype), sigma2.to(X.device, X.dtype)
        pi, mu, W, sigma2, R, L, _ = mppca_gem_torch(X, pi, mu, W, sigma2, N_ITERATIONS, batch_size=BATCH_SIZE, device=device)
        
        final_log_L = L[-1].item() if torch.isfinite(L[-1]) else 'NaN'
        logging.info(f"学習完了。最終的な対数尤度: {final_log_L}")
        
        model_path = os.path.join(RESULT_DIR, 'mppca_model.pt'); 
        torch.save({'pi': pi, 'mu': mu, 'W': W, 'sigma2': sigma2, 'R': R, 'lat': lat_coords, 'lon': lon_coords, 'data_mean': data_mean}, model_path)
        logging.info(f"学習済みモデルを保存: {model_path}")

        logging.info("--- 結果の可視化と分析を開始 ---")
        plot_log_likelihood(L, os.path.join(RESULT_DIR, 'log_likelihood.png'))
        plot_average_patterns(mu, lat_coords, lon_coords, data_mean, os.path.join(RESULT_DIR, 'average_patterns.png'))
        latent_coords, cluster_assignments = calculate_latent_coords(X, R, mu, W, sigma2, device)
        plot_latent_space(latent_coords, cluster_assignments, P_CLUSTERS, os.path.join(RESULT_DIR, 'latent_space.png'))

        logging.info("--- 個々のデータに着目した結果の分析 ---")
        N_SAMPLES_TO_ANALYZE = 5
        logging.info(f"最初の{N_SAMPLES_TO_ANALYZE}個のサンプルについて、詳細な結果を表示します。")
        for i in range(min(N_SAMPLES_TO_ANALYZE, len(X))):
            date_str = pd.to_datetime(str(time_stamps[i])).strftime('%Y-%m-%d'); r_i = R[i]
            assigned_cluster = cluster_assignments[i].item(); max_prob = r_i[assigned_cluster].item()
            latent_coords_i = latent_coords[i].cpu().numpy()
            
            logging.info(f"\n[サンプル {i+1} ({date_str})]")
            logging.info(f"  - 最も可能性の高いクラスター: {assigned_cluster + 1} (確率: {max_prob:.4f})")
            
            latent_coords_str = format_coord_vector(latent_coords_i)
            logging.info(f"  - (主)潜在空間での座標 (次元数: {len(latent_coords_i)}):\n {latent_coords_str}")

            prob_vector_str = ", ".join([f"{p:.4f}" for p in r_i.cpu().numpy()])
            logging.info(f"  - 全クラスターへの所属確率ベクトル [R_i]: [{prob_vector_str}]")
            
            all_coords_i = calculate_all_latent_coords_for_sample(X[i], mu, W, sigma2, device)
            logging.info(f"  - 全{P_CLUSTERS}個の潜在空間での座標:")
            for c in range(P_CLUSTERS):
                coord_c = all_coords_i[c].cpu().numpy()
                coord_c_str = format_coord_vector(coord_c)
                logging.info(f"    - Cluster {c+1:>2} の場合:\n {coord_c_str}")

        plot_reconstructions(X, mu, W, latent_coords, cluster_assignments, lat_coords, lon_coords, data_mean, time_stamps, N_SAMPLES_TO_ANALYZE, os.path.join(RESULT_DIR, 'reconstructions.png'))
        logging.info(f"すべての結果は '{RESULT_DIR}' ディレクトリに保存されました。")

    logging.info("--- プログラム終了 ---")
```

/home/takumi/docker_miniconda/src/PressurePattern/SOM-MPPCA/mppca_pytorch.py
```
import torch
import math
from tqdm import tqdm


def initialization_kmeans_torch(X, p, q, variance_level=None, device='cpu'):
    """
    Initializes MPPCA parameters using k-means clustering.
    Args:
        X (torch.Tensor): The dataset of shape (N, d). Expected to be on the target device.
        p (int): Number of clusters.
        q (int): Dimension of the latent space.
        variance_level (float, optional): A factor for initializing W and sigma2. Defaults to None.
        device (str): The device to run computations on ('cpu' or 'cuda').

    Returns:
        tuple: (pi, mu, W, sigma2, clusters)
    """
    N, d = X.shape
    # X is assumed to be on the correct device already.

    # Randomly select initial cluster centers without replacement
    indices = torch.randperm(N, device=device)[:p]
    mu = X[indices]

    clusters = torch.zeros(N, dtype=torch.long, device=device)
    D_old = -2.0
    D = -1.0

    # K-means clustering loop
    # This loop is kept simple for initialization. More robust k-means could be used.
    for _ in range(100): # Use a fixed number of iterations to prevent infinite loops
        # Assign clusters based on the closest center (Euclidean distance)
        distance_square = torch.cdist(X, mu) ** 2
        clusters = torch.argmin(distance_square, axis=1)

        # Compute distortion (sum of squared distances to the assigned center)
        distmin = distance_square[torch.arange(N, device=device), clusters]
        current_D = distmin.sum().item()
        
        # Check for convergence
        if D_old == current_D:
            break
        D_old = current_D

        # Update centers to be the mean of the points in each cluster
        for c in range(p):
            cluster_points = X[clusters == c]
            if len(cluster_points) > 0:
                mu[c] = cluster_points.mean(0)
    
    D = distmin.sum().item()

    # Initialize MPPCA parameters based on k-means results
    pi = torch.zeros(p, device=device, dtype=X.dtype)
    W = torch.zeros((p, d, q), device=device, dtype=X.dtype)
    sigma2 = torch.zeros(p, device=device, dtype=X.dtype)

    for c in range(p):
        cluster_mask = (clusters == c)
        num_points = cluster_mask.sum().item()
        
        if num_points == 0:
            # Handle empty clusters if they occur
            pi[c] = 1e-9 # Assign a very small probability
            # Re-initialize mu for the empty cluster to a random point
            mu[c] = X[torch.randint(0, N, (1,)).item()]
            W[c, :, :] = torch.randn(d, q, device=device, dtype=X.dtype) * 0.1
            sigma2[c] = torch.var(X) / d # Use global variance
            continue

        pi[c] = num_points / N

        if variance_level is not None:
            W[c, :, :] = variance_level * torch.randn(d, q, device=device, dtype=X.dtype)
            sigma2[c] = torch.abs((variance_level / 10) * torch.randn(1, device=device, dtype=X.dtype))
        else:
            # Initialize W with small random values
            W[c, :, :] = torch.randn(d, q, device=device, dtype=X.dtype) * 0.1
            # Initialize sigma2 based on the in-cluster variance
            sigma2[c] = (distmin[cluster_mask].mean() / d) + 1e-6

    return pi, mu, W, sigma2, clusters


def mppca_gem_torch(X, pi, mu, W, sigma2, niter, batch_size=1024, device='cpu'):
    """
    Performs the Generalized Expectation-Maximization (GEM) algorithm for MPPCA.
    
    Args:
        X, pi, mu, W, sigma2: Tensors for data and parameters.
        These are expected to be on the target device and of dtype float64 for stability.
    """
    N, d = X.shape
    p, _, q = W.shape
    
    # Epsilon: A small constant to prevent division by zero or log(0)
    epsilon = torch.finfo(X.dtype).eps

    # Tensors for tracking history, created with the same dtype as X
    sigma2hist = torch.zeros((p, niter), device=device, dtype=X.dtype)
    L = torch.zeros(niter, device=device, dtype=X.dtype)
    I_q = torch.eye(q, device=device, dtype=X.dtype)

    pbar = tqdm(range(niter), desc="GEM Algorithm Progress")
    for i in pbar:
        sigma2hist[:, i] = sigma2

        # --- E-step (Expectation) ---
        logR = torch.zeros((N, p), device=device, dtype=X.dtype)
        
        W_T = W.transpose(-2, -1)
        # Add a small value to sigma2 on the diagonal for robustness
        M = (sigma2.view(p, 1, 1) + epsilon) * I_q + torch.bmm(W_T, W)
        
        try:
            M_inv = torch.linalg.inv(M)
            log_det_M = torch.linalg.slogdet(M).logabsdet
        except torch.linalg.LinAlgError:
            # If M is singular, add a small identity matrix for regularization
            M_inv = torch.linalg.inv(M + torch.eye(q, device=device, dtype=X.dtype) * 1e-6)
            log_det_M = torch.linalg.slogdet(M + torch.eye(q, device=device, dtype=X.dtype) * 1e-6).logabsdet

        # Pre-calculate the log determinant term of the inverse covariance
        log_det_C_inv_half = -0.5 * (log_det_M + (d - q) * torch.log(sigma2 + epsilon))

        # Process data in batches to manage memory usage
        for batch_start in range(0, N, batch_size):
            batch_end = min(batch_start + batch_size, N)
            X_batch = X[batch_start:batch_end]
            
            deviation = X_batch.unsqueeze(0) - mu.unsqueeze(1) # Shape: (p, batch_size, d)
            
            M_inv_W_T = torch.bmm(M_inv, W_T)
            temp = torch.bmm(deviation, W)
            temp = torch.bmm(temp, M_inv_W_T)
            
            # Mahalanobis-like distance term
            quadratic_form = (torch.sum(deviation**2, dim=-1) - torch.sum(deviation * temp, dim=-1)) / sigma2.view(p, 1)

            # Calculate log responsibility (unnormalized)
            logR[batch_start:batch_end, :] = (
                torch.log(pi + epsilon) # Add epsilon to prevent log(0)
                + log_det_C_inv_half
                - 0.5 * quadratic_form.T
            )

        # --- Log-likelihood Calculation (for monitoring convergence) ---
        myMax = torch.max(logR, axis=1, keepdim=True).values
        # Handle cases where logR might become -inf
        myMax = torch.nan_to_num(myMax, neginf=-1e30)

        logR_stable = logR - myMax
        log_sum_exp = torch.log(torch.exp(logR_stable).sum(axis=1) + epsilon)
        L[i] = (myMax.squeeze() + log_sum_exp).sum()
        L[i] -= N * d * math.log(2 * math.pi) / 2.0
        
        pbar.set_postfix(log_likelihood=f"{L[i].item():.4f}")

        # Normalize logR to get R (posterior probabilities) using log-sum-exp trick
        log_sum_R = torch.logsumexp(logR, dim=1, keepdim=True)
        logR = logR - log_sum_R
        R = torch.exp(logR)

        # Check for NaN/inf in responsibilities, which indicates a fatal error
        if not torch.all(torch.isfinite(R)):
            print(f"\nFATAL: Responsibilities contain NaN/inf at iteration {i}. Stopping.")
            L[i:] = float('nan')
            break
            
        # --- M-step (Maximization) ---
        R_sum = R.sum(axis=0)
        R_sum_stable = R_sum + (10 * epsilon)

        # Update pi (mixing proportions)
        pi = R_sum / N

        # Update mu (cluster means)
        mu = torch.einsum('np,nd->pd', R, X) / R_sum_stable.unsqueeze(1)

        # Update W and sigma2 (factor loadings and noise variance)
        S_W_numerator = torch.zeros((p, d, q), device=device, dtype=X.dtype)
        term2_numerator = torch.zeros(p, device=device, dtype=X.dtype)
        
        for batch_start in range(0, N, batch_size):
            batch_end = min(batch_start + batch_size, N)
            X_batch = X[batch_start:batch_end]
            R_batch = R[batch_start:batch_end]
            
            deviation = X_batch.unsqueeze(0) - mu.unsqueeze(1)
            
            R_dev_T = (R_batch.T.unsqueeze(-1) * deviation).transpose(-2, -1)
            dev_W = torch.bmm(deviation, W)
            S_W_numerator += torch.bmm(R_dev_T, dev_W)
            
            term2_numerator += torch.einsum('np,pnd->p', R_batch, deviation**2)

        S_W = S_W_numerator / R_sum_stable.view(p, 1, 1)
        
        try:
            inner_term = (sigma2.view(p, 1, 1) * I_q) + torch.bmm(torch.bmm(M_inv, W_T), S_W)
            inner_inv = torch.linalg.inv(inner_term)
        except torch.linalg.LinAlgError:
            # Regularize if inner term is singular
            inner_term_reg = inner_term + torch.eye(q, device=device, dtype=X.dtype) * 1e-6
            inner_inv = torch.linalg.inv(inner_term_reg)
            
        W_new = torch.bmm(S_W, inner_inv)

        trace_term = torch.einsum('pdq,pqi->pdi', S_W, M_inv)
        trace_term = torch.einsum('pji,pij->p', W_new.transpose(-2,-1), trace_term)

        sigma2_new = (1 / d) * ( (term2_numerator / R_sum_stable) - trace_term )
        
        W = W_new
        sigma2 = sigma2_new

        # --- ▼▼▼【重要】クラスター崩壊防止と再初期化 ▼▼▼ ---
        # If a cluster's responsibility (pi) becomes too small, it has "collapsed".
        # We re-initialize it to give it a chance to capture a new part of the data.
        pi_floor = 1.0 / (N * 10) # Define a minimum responsibility threshold
        dead_clusters_mask = (pi < pi_floor)

        if torch.any(dead_clusters_mask):
            # Find the average variance of healthy clusters for a sensible initialization
            healthy_sigma2_mean = sigma2[~dead_clusters_mask].mean()
            if not torch.isfinite(healthy_sigma2_mean):
                healthy_sigma2_mean = torch.var(X) # Fallback if all clusters are dead

            for c_idx in torch.where(dead_clusters_mask)[0]:
                print(f"\nINFO: Re-initializing collapsed cluster {c_idx.item()} at iteration {i}.")
                # 1. Re-initialize mu to a random data point
                random_idx = torch.randint(0, N, (1,)).item()
                mu[c_idx] = X[random_idx]
                
                # 2. Re-initialize W with small random values
                W[c_idx] = torch.randn_like(W[c_idx]) * 0.1
                
                # 3. Reset the noise variance to a sensible value
                sigma2[c_idx] = healthy_sigma2_mean
                
                # 4. Give the cluster a small amount of "life" by taking from the richest cluster
                if torch.any(~dead_clusters_mask):
                    richest_cluster_idx = torch.argmax(pi)
                    stolen_pi = min(pi[richest_cluster_idx] * 0.1, 0.01) # Steal up to 1%
                    pi[c_idx] += stolen_pi
                    pi[richest_cluster_idx] -= stolen_pi

            # Re-normalize pi after adjustment
            pi = torch.clamp(pi, min=epsilon)
            pi /= pi.sum()
        # --- ▲▲▲ 修正ここまで ▲▲▲ ---

        # Finally, ensure sigma2 does not become negative or zero
        sigma2 = torch.clamp(sigma2, min=epsilon)

    pbar.close()
    
    # Final check for any remaining NaNs in results
    if not torch.all(torch.isfinite(L)):
        print("\nWarning: Log-likelihood contains NaN values after training.")
        
    print("\nDone.")
    return pi, mu, W, sigma2, R, L, sigma2hist
```

/home/takumi/docker_miniconda/src/PressurePattern/SOM-MPPCA/minisom.py
```
from numpy import (array, unravel_index, nditer, linalg, random, subtract, max,
                   power, exp, zeros, ones, arange, outer, meshgrid, dot,
                   logical_and, mean, cov, argsort, linspace,
                   einsum, prod, nan, sqrt, hstack, diff, argmin, multiply,
                   nanmean, nansum, tile, array_equal, isclose, bincount)
from numpy.linalg import norm
from collections import defaultdict, Counter
from warnings import warn
import warnings
from sys import stdout
from time import time
from datetime import timedelta
import pickle
import os
import numba
from numba import jit

"""
    Minimalistic implementation of the Self Organizing Maps (SOM).
"""


def _build_iteration_indexes(data_len, num_iterations,
                             verbose=False, random_generator=None,
                             use_epochs=False):
    """Returns an iterable with the indexes of the samples
    to pick at each iteration of the training.

    If random_generator is not None, it must be an instance
    of numpy.random.RandomState and it will be used
    to randomize the order of the samples."""
    if use_epochs:
        iterations_per_epoch = arange(data_len)
        if random_generator:
            random_generator.shuffle(iterations_per_epoch)
        iterations = tile(iterations_per_epoch, num_iterations)
    else:
        iterations = arange(num_iterations) % data_len
        if random_generator:
            random_generator.shuffle(iterations)
    if verbose:
        # tqdm互換の進捗表示に変更
        from tqdm import trange
        return trange(num_iterations)
    else:
        return range(num_iterations)


def _wrap_index__in_verbose(iterations):
    """(tqdmに置き換えられたため、この関数は直接は使われない)"""
    m = len(iterations)
    digits = len(str(m))
    progress = '\r [ {s:{d}} / {m} ] {s:3.0f}% - ? it/s'
    progress = progress.format(m=m, d=digits, s=0)
    stdout.write(progress)
    beginning = time()
    stdout.write(progress)
    for i, it in enumerate(iterations):
        yield it
        sec_left = ((m-i+1) * (time() - beginning)) / (i+1)
        time_left = str(timedelta(seconds=sec_left))[:7]
        progress = '\r [ {i:{d}} / {m} ]'.format(i=i+1, d=digits, m=m)
        progress += ' {p:3.0f}%'.format(p=100*(i+1)/m)
        progress += ' - {time_left} left '.format(time_left=time_left)
        stdout.write(progress)


# NumbaのJITデコレータを追加して高速化
@jit(nopython=True, cache=True)
def fast_norm(x):
    """Returns norm-2 of a 1-D numpy array.
    """
    return sqrt(dot(x, x.T))


# Numba JITコンパイル用のヘルパー関数 (distance_mapのループ部分)
@jit(nopython=True, cache=True)
def _distance_map_loop(weights, um, ii, jj):
    """Numba-accelerated loop for distance_map."""
    for x in range(weights.shape[0]):
        for y in range(weights.shape[1]):
            # nanチェックを追加
            if not isnan(weights[x, y, 0]):
                w_2 = weights[x, y]
                # ★★★ 修正点 3: ブール値のインデックスを整数に明示的に変換 ★★★
                e = y % 2 == 0
                row_index = int(e) 
                for k, (i, j) in enumerate(zip(ii[row_index], jj[row_index])):
                    if (x + i >= 0 and x + i < weights.shape[0] and
                            y + j >= 0 and y + j < weights.shape[1]):
                        # 隣接ノードの重みもnanでないことを確認
                        if not isnan(weights[x + i, y + j, 0]):
                            w_1 = weights[x + i, y + j]
                            um[x, y, k] = fast_norm(w_2 - w_1)
    return um

# isnanをnopythonモードで使えるようにJIT化
@jit(nopython=True, cache=True)
def isnan(x):
    return x != x

class MiniSom(object):
    Y_HEX_CONV_FACTOR = (3.0 / 2.0) / sqrt(3)

    def __init__(self, x, y, input_len, sigma=1, learning_rate=0.5,
                 decay_function='asymptotic_decay',
                 neighborhood_function='gaussian', topology='rectangular',
                 activation_distance='euclidean', random_seed=None,
                 sigma_decay_function='asymptotic_decay'):
        if sigma > sqrt(x*x + y*y):
            warn('Warning: sigma might be too high ' +
                 'for the dimension of the map.')

        self._random_generator = random.RandomState(random_seed)

        self._learning_rate = learning_rate
        self._sigma = sigma
        self._input_len = input_len
        self._weights = self._random_generator.rand(x, y, input_len)*2-1
        self._weights /= linalg.norm(self._weights, axis=-1, keepdims=True)

        self._activation_map = zeros((x, y))
        self._neigx = arange(x, dtype=float)
        self._neigy = arange(y, dtype=float)

        if topology not in ['hexagonal', 'rectangular']:
            msg = '%s not supported only hexagonal and rectangular available'
            raise ValueError(msg % topology)
        self.topology = topology
        self._xx, self._yy = meshgrid(self._neigx, self._neigy)
        self._xx = self._xx.astype(float)
        self._yy = self._yy.astype(float)
        if topology == 'hexagonal':
            self._xx[1::2] -= 0.5 # 修正: 奇数行をずらす正しい方法
            self._yy *= self.Y_HEX_CONV_FACTOR
            if neighborhood_function in ['triangle']:
                warn('triangle neighborhood function does not ' +
                     'take in account hexagonal topology')

        lr_decay_functions = {
            'inverse_decay_to_zero': self._inverse_decay_to_zero,
            'linear_decay_to_zero': self._linear_decay_to_zero,
            'asymptotic_decay': self._asymptotic_decay}

        if isinstance(decay_function, str):
            if decay_function not in lr_decay_functions:
                msg = '%s not supported. Functions available: %s'
                raise ValueError(msg % (decay_function,
                                        ', '.join(lr_decay_functions.keys())))
            self._learning_rate_decay_function = lr_decay_functions[decay_function]
        elif callable(decay_function):
            self._learning_rate_decay_function = decay_function

        sig_decay_functions = {
            'inverse_decay_to_one': self._inverse_decay_to_one,
            'linear_decay_to_one': self._linear_decay_to_one,
            'asymptotic_decay': self._asymptotic_decay}

        if sigma_decay_function not in sig_decay_functions:
            msg = '%s not supported. Functions available: %s'
            raise ValueError(msg % (sigma_decay_function,
                                    ', '.join(sig_decay_functions.keys())))
        self._sigma_decay_function = sig_decay_functions[sigma_decay_function]

        neig_functions = {'gaussian': self._gaussian,
                          'mexican_hat': self._mexican_hat,
                          'bubble': self._bubble,
                          'triangle': self._triangle}

        if neighborhood_function not in neig_functions:
            msg = '%s not supported. Functions available: %s'
            raise ValueError(msg % (neighborhood_function,
                                    ', '.join(neig_functions.keys())))
        self.neighborhood = neig_functions[neighborhood_function]

        distance_functions = {'euclidean': self._euclidean_distance,
                              'cosine': self._cosine_distance,
                              'manhattan': self._manhattan_distance,
                              'chebyshev': self._chebyshev_distance}

        if isinstance(activation_distance, str):
            if activation_distance not in distance_functions:
                msg = '%s not supported. Distances available: %s'
                raise ValueError(msg % (activation_distance,
                                        ', '.join(distance_functions.keys())))
            self._activation_distance = distance_functions[activation_distance]
        elif callable(activation_distance):
            self._activation_distance = activation_distance

    def get_weights(self):
        return self._weights

    def get_euclidean_coordinates(self):
        return self._xx.T, self._yy.T

    def convert_map_to_euclidean(self, xy):
        return self._xx.T[xy], self._yy.T[xy]

    def _activate(self, x):
        self._activation_map = self._activation_distance(x, self._weights)

    def activate(self, x):
        self._activate(x)
        return self._activation_map

    def _inverse_decay_to_zero(self, learning_rate, t, max_iter):
        C = max_iter / 100.0
        return learning_rate * C / (C + t)

    def _linear_decay_to_zero(self, learning_rate, t, max_iter):
        return learning_rate * (1 - t / max_iter)

    def _inverse_decay_to_one(self, sigma, t, max_iter):
        C = (sigma - 1) / max_iter
        return sigma / (1 + (t * C))

    def _linear_decay_to_one(self, sigma, t, max_iter):
        return sigma + (t * (1 - sigma) / max_iter)

    def _asymptotic_decay(self, dynamic_parameter, t, max_iter):
        return dynamic_parameter / (1 + t / (max_iter / 2))

    def _gaussian(self, c, sigma):
        d = 2*sigma*sigma
        ax = exp(-power(self._xx-self._xx.T[c], 2)/d)
        ay = exp(-power(self._yy-self._yy.T[c], 2)/d)
        return (ax * ay).T

    def _mexican_hat(self, c, sigma):
        p = power(self._xx-self._xx.T[c], 2) + power(self._yy-self._yy.T[c], 2)
        d = 2*sigma*sigma
        return (exp(-p/d)*(1-2/d*p)).T

    def _bubble(self, c, sigma):
        ax = logical_and(self._neigx > c[0]-sigma,
                         self._neigx < c[0]+sigma)
        ay = logical_and(self._neigy > c[1]-sigma,
                         self._neigy < c[1]+sigma)
        return outer(ax, ay)*1.

    def _triangle(self, c, sigma):
        triangle_x = (-abs(c[0] - self._neigx)) + sigma
        triangle_y = (-abs(c[1] - self._neigy)) + sigma
        triangle_x[triangle_x < 0] = 0.
        triangle_y[triangle_y < 0] = 0.
        return outer(triangle_x, triangle_y)

    def _cosine_distance(self, x, w):
        num = (w * x).sum(axis=2)
        denum = multiply(linalg.norm(w, axis=2), linalg.norm(x))
        return 1 - num / (denum+1e-8)

    def _euclidean_distance(self, x, w):
        return linalg.norm(subtract(x, w), axis=-1)

    def _manhattan_distance(self, x, w):
        return linalg.norm(subtract(x, w), ord=1, axis=-1)

    def _chebyshev_distance(self, x, w):
        return max(subtract(x, w), axis=-1)

    def _check_iteration_number(self, num_iteration):
        if num_iteration < 1:
            raise ValueError('num_iteration must be > 1')

    def _check_input_len(self, data):
        data_len = len(data[0])
        if self._input_len != data_len:
            msg = 'Received %d features, expected %d.' % (data_len,
                                                          self._input_len)
            raise ValueError(msg)

    def winner(self, x):
        self._activate(x)
        # 修正点: 2次元の activation_map を .flatten() で1次元配列に変換する
        return unravel_index(nanargmin(self._activation_map.flatten()),
                             self._activation_map.shape)

    def update(self, x, win, t, max_iteration):
        eta = self._learning_rate_decay_function(self._learning_rate,
                                                 t, max_iteration)
        sig = self._sigma_decay_function(self._sigma, t, max_iteration)
        g = self.neighborhood(win, sig)*eta
        self._weights += einsum('ij, ijk->ijk', g, x-self._weights)

    def quantization(self, data):
        self._check_input_len(data)
        # 修正点: nanargmin を NumPy の argmin に変更
        winners_coords = argmin(self._distance_from_weights(data), axis=1)
        return self._weights[unravel_index(winners_coords,
                                           self._weights.shape[:2])]

    def random_weights_init(self, data):
        self._check_input_len(data)
        map_shape = self._weights.shape[:2]
        rand_indices = self._random_generator.randint(len(data),
                                                      size=map_shape)
        self._weights = array(data)[rand_indices]

    def pca_weights_init(self, data):
        if self._input_len == 1:
            msg = 'The data needs at least 2 features for pca initialization'
            raise ValueError(msg)
        self._check_input_len(data)
        if len(self._neigx) == 1 or len(self._neigy) == 1:
            msg = 'PCA initialization inappropriate:' + \
                  'One of the dimensions of the map is 1.'
            warn(msg)
        pc_length, pc_vecs = linalg.eigh(cov(data.T))
        pc_order = argsort(-pc_length)
        pc_vecs = pc_vecs[:, pc_order]
        mx, my = self._weights.shape[0], self._weights.shape[1]
        c1 = linspace(-1, 1, mx)[:, None]
        c2 = linspace(-1, 1, my)
        pc1 = data.mean(axis=0) + c1 * sqrt(pc_length[pc_order[0]]) * pc_vecs[:, 0]
        pc2 = c2 * sqrt(pc_length[pc_order[1]]) * pc_vecs[:, 1]
        self._weights = pc1[:, None, :] + pc2[None, :, :]

    def train(self, data, num_iteration,
              random_order=False, verbose=False,
              use_epochs=False):
        data = array(data)
        self._check_input_len(data)
        # nanが含まれている場合、学習に影響が出るため警告
        if isnan(data.sum()):
             warn('Input data contains nan values. This will influence the learning process.')

        random_generator = self._random_generator if random_order else None
        
        # ログとtqdmの重複を避けるため、verbose=Trueでもtqdmを使うように
        iterations = _build_iteration_indexes(len(data), num_iteration,
                                              True, random_generator,
                                              use_epochs)

        if use_epochs:
            def get_decay_rate(iteration_index, data_len):
                return int(iteration_index / data_len)
        else:
            def get_decay_rate(iteration_index, data_len):
                return int(iteration_index)

        data_len = len(data)
        for t, iteration_index in enumerate(iterations):
            # 実際のデータインデックスを取得
            data_idx = iteration_index % data_len
            x = data[data_idx]
            # 入力データがnanの場合はスキップ
            if isnan(x.sum()):
                continue
            
            decay_rate = get_decay_rate(t, data_len)
            self.update(x, self.winner(x), decay_rate, num_iteration)
        
        # tqdmを使う場合、quantization errorの表示はmain側で行う
        q_error = self.quantization_error(data)
        if verbose:
            print(f'\n quantization error: {q_error}')

    def train_batch(self, data, num_iteration, verbose=False, log_interval=1000):
        """
        バッチ学習を用いてSOMを訓練します。
        
        :param data: NxD のNumpy配列。Nはサンプル数、Dは特徴次元数。
        :param num_iteration: 学習の反復回数。
        :param verbose: 進捗を表示するかどうか。
        :param log_interval: 量子化誤差を記録する間隔。
        :return: 量子化誤差の履歴リスト。
        """
        self._check_iteration_number(num_iteration)
        self._check_input_len(data)

        quantization_errors = [] # 誤差を記録するリストを初期化

        # verboseがTrueの場合、tqdmによるプログレスバーを表示
        iterations = _build_iteration_indexes(len(data), num_iteration,
                                                verbose=verbose, use_epochs=False)

        for i in iterations:
            # 学習率と近傍半径を更新
            eta = self._learning_rate_decay_function(self._learning_rate, i, num_iteration)
            sig = self._sigma_decay_function(self._sigma, i, num_iteration)

            # E-step: 各データに最も近いノード（勝者ノード）を見つける
            win_map = self.win_map(data)

            # M-step: 重みを更新する
            numerator = zeros(self._weights.shape)
            denominator = zeros(self._weights.shape[:2])

            # バッチ更新のための分子と分母を計算
            for win_pos, data_points in win_map.items():
                h = self.neighborhood(win_pos, sig)
                numerator += h[:, :, None] * mean(data_points, axis=0)
                denominator += h
            
            # 重みを更新
            self._weights = numerator / (denominator[:, :, None] + 1e-9)
            
            # 指定された間隔で量子化誤差を記録
            if i % log_interval == 0:
                q_error = self.quantization_error(data)
                quantization_errors.append(q_error)
                # tqdmの進捗バーに現在の誤差を表示
                if verbose and hasattr(iterations, 'set_postfix'):
                    iterations.set_postfix(q_error=f"{q_error:.5f}")

        if verbose:
            q_error = self.quantization_error(data)
            print(f'\n final quantization error: {q_error}')
        
        # 量子化誤差の履歴を返す
        return quantization_errors


    def distance_map(self, scaling='sum'):
        if scaling not in ['sum', 'mean']:
            raise ValueError(f'scaling should be either "sum" or "mean" ('
                             f'"{scaling}" not valid)')
        
        weights = self._weights
        um = nan * zeros((weights.shape[0], weights.shape[1], 8))

        if self.topology == 'hexagonal':
            ii = array([[1, 1, 1, 0, -1, 0], [0, 1, 0, -1, -1, -1]], dtype=int)
            jj = array([[1, 0, -1, -1, 0, 1], [1, 0, -1, -1, 0, 1]], dtype=int)
        else: # rectangular
            ii = array([[0, -1, -1, -1, 0, 1, 1, 1], [0, -1, -1, -1, 0, 1, 1, 1]], dtype=int)
            jj = array([[-1, -1, 0, 1, 1, 1, 0, -1], [-1, -1, 0, 1, 1, 1, 0, -1]], dtype=int)
        
        um = _distance_map_loop(weights, um, ii, jj)
        
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", category=RuntimeWarning)
            if scaling == 'mean':
                um = nanmean(um, axis=2)
            if scaling == 'sum':
                um = nansum(um, axis=2)
            
        # nanが含まれる場合、最大値計算でエラーになるのを防ぐ
        max_val = nanmax(um)
        if max_val == 0 or isnan(max_val):
            return um
        else:
            return um / max_val

    def activation_response(self, data):
        self._check_input_len(data)
        data_clean = data[~isnan(data).any(axis=1)]
        if len(data_clean) == 0:
            return zeros(self._weights.shape[:2])
            
        map_shape = self._weights.shape[:2]
        winners_flat = self._distance_from_weights(data_clean).argmin(axis=1)
        win_counts = bincount(winners_flat, minlength=prod(map_shape))
        return win_counts.reshape(map_shape)

    def _distance_from_weights(self, data):
        input_data = array(data)
        weights_flat = self._weights.reshape(-1, self._weights.shape[2])
        input_data_sq = power(input_data, 2).sum(axis=1, keepdims=True)
        weights_flat_sq = power(weights_flat, 2).sum(axis=1, keepdims=True)
        cross_term = dot(input_data, weights_flat.T)
        return sqrt(maximum(0.0, -2 * cross_term + input_data_sq + weights_flat_sq.T))

    def quantization_error(self, data):
        self._check_input_len(data)
        data_clean = data[~isnan(data).any(axis=1)]
        if len(data_clean) == 0:
            return nan
        return norm(data_clean-self.quantization(data_clean), axis=1).mean()
        
    def win_map(self, data, return_indices=False):
        self._check_input_len(data)
        winmap = defaultdict(list)
        # nanデータを除外して処理
        valid_indices = where(~isnan(data).any(axis=1))[0]
        if len(valid_indices) == 0:
            return winmap

        winners = self._winners_from_weights(data[valid_indices])
        
        data_to_append = valid_indices if return_indices else data[valid_indices]

        for i, win_pos in enumerate(winners):
            winmap[win_pos].append(data_to_append[i])
        return winmap
        
    def _winners_from_weights(self, data):
        """Helper function to return the winner coordinates for all data points."""
        distances = self._distance_from_weights(data)
        # 修正点: nanargmin を NumPy の argmin に変更
        winner_indices = argmin(distances, axis=1)
        return [unravel_index(i, self._weights.shape[:2]) for i in winner_indices]

# nanを無視するargminとnanmaxをJIT化
@jit(nopython=True, cache=True)
def nanargmin(arr):
    min_val = float('inf')
    min_idx = -1
    for i in range(arr.shape[0]):
        if not isnan(arr[i]) and arr[i] < min_val:
            min_val = arr[i]
            min_idx = i
    return min_idx

from numpy import nanmax, where, maximum
```


まずは上記のプログラムを完全に理解してください
上記のプログラムを全て活用した研究を行いたいです
行いたい研究、および研究計画は以下になります

複雑な海面更生気圧からパターン（夏型、冬型、台風、梅雨など...）を抽出して、綺麗に分類する研究を行っています
使用するデータは以下になります

① 広い範囲の海面更生気圧データ
範囲：55, 115, 15, 155（North, West, South, East）
./prmsl_era5_all_data_seasonal_large.nc

② 標準サイズの海面更生気圧データ
範囲：50, 120, 20, 150（North, West, South, East）
./prmsl_era5_all_data_seasonal_normal.nc

③ 狭い範囲の海面更生気圧データ
範囲：45, 125, 25, 145（North, West, South, East）
./prmsl_era5_all_data_seasonal_small.nc

これらのデータを元に以下の実験を行いたいです
1. MPPCA+SOM_1
MPPCAでクラスタ数15つで20次元

2. MPPCA+SOM_2
MPPCAでクラスタ数6つで20次元

3. MPPCA+SOM_3
MPPCAでクラスタ数4つで20次元

4. PCA+SOM_1
PCAで20次元に圧縮
SOMをかける

5. PCA+SOM_2
PCAで80次元に圧縮
SOMをかける

6. PCA+SOM_3
PCAで120次元に圧縮
SOMをかける

私が行ってほしいこととして、main_v2_pytorch.pyとmain.pyの要素を組み合わせて、main_v3.pyを作成してほしいです。
minisom.pyとmppca_pytorch.pyを改善する場合は、修正、改善を行う該当箇所について修正前と修正後の表示を行なってほしいです。
main_v3.pyに関しては完璧なプログラムを作成して全文表示してください
どんなプログラムを作成したのかは日本語で詳しく解説してください

3通り（データの数）×6通り（圧縮の方法やパラメータ）が考えられます
上記のように3種類のデータ（large, normal, small）、2種類の圧縮方法（PCA, MPPCA）、それらのパラメータの数などを可変できるようなプログラムがいいです
例：以下のような感じで実験したいパラメータでセットできる
```
# --- 実行する実験のリスト ---
# name: 結果を保存するディレクトリ名
# data_key: 使用するデータ (DATA_FILESのキー)
# method: 'MPPCA' または 'PCA'
# p_clusters: MPPCAのクラスタ数 (MPPCAの場合のみ有効)
# q_latent_dim: MPPCAの潜在次元数 (MPPCAの場合のみ有効)
# n_components: PCAの主成分数 (PCAの場合のみ有効)
EXPERIMENTS = [
    # --- 【例】smallデータセットでの実験 ---
    {'name': 'MPPCA_p15_q20_small', 'data_key': 'small', 'method': 'MPPCA', 'p_clusters': 15, 'q_latent_dim': 20},
    {'name': 'MPPCA_p6_q20_small',  'data_key': 'small', 'method': 'MPPCA', 'p_clusters': 6,  'q_latent_dim': 20},
    {'name': 'MPPCA_p4_q20_small',  'data_key': 'small', 'method': 'MPPCA', 'p_clusters': 4,  'q_latent_dim': 20},
    {'name': 'PCA_d20_small',       'data_key': 'small', 'method': 'PCA',   'n_components': 20},
    {'name': 'PCA_d80_small',       'data_key': 'small', 'method': 'PCA',   'n_components': 80},
    {'name': 'PCA_d120_small',      'data_key': 'small', 'method': 'PCA',   'n_components': 120},
    
    # --- 【例】normalデータセットでの実験 (必要に応じてコメントアウトを解除) ---
    # {'name': 'MPPCA_p15_q20_normal', 'data_key': 'normal', 'method': 'MPPCA', 'p_clusters': 15, 'q_latent_dim': 20},
    # {'name': 'PCA_d80_normal',       'data_key': 'normal', 'method': 'PCA',   'n_components': 80},
    
    # --- 【例】largeデータセットでの実験 (必要に応じてコメントアウトを解除) ---
    # {'name': 'MPPCA_p15_q20_large', 'data_key': 'large', 'method': 'MPPCA', 'p_clusters': 15, 'q_latent_dim': 20},
    # {'name': 'PCA_d80_large',       'data_key': 'large', 'method': 'PCA',   'n_components': 80},
]
```
プログラムは research_results_v3 というディレクトリを作成し、その中に各実験名のサブディレクトリを作成して結果を保存する感じ。
それぞれの実行ログもディレクトリ内に保存するようにして下さい。

全体にいる必要な要素
- 再現性のためのシード
- Python標準の`logging`モジュールを導入し、コンソールとファイルにログを記録
- マクロ平均再現率の使用
- 現在main.pyで出している可視化やログは引き続き出すこと
- mppca、somにおいてN_ITERATIONSやN_ITER_SOMがあるが、人力で決めたくないので、収束を観測したら自動で止める処理を追加してほしいです（最大反復回数は設定可能）
- ラベルの情報
基本ラベルが以下のようにありますが、
BASE_LABELS = [
    '1', '2A', '2B', '2C', '2D', '3A', '3B', '3C', '3D',
    '4A', '4B', '5', '6A', '6B', '6C'
]
複合ラベルというものもあります
+か-で2つの基本ラベルがくっついたもので（例：1+2A）、この場合の扱いはmain.pyを踏襲してください。
- データにおいて使用する期間
以下の期間以外はラベルがないので、以下の期間を絶対に使用するようにしてください
# データ期間
START_DATE = '1991-01-01'
END_DATE = '2000-12-31'

